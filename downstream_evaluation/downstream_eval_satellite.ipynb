{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5355b9b7",
   "metadata": {},
   "source": [
    "# Downstream Evaluation: Land Cover Classification from Satellite Data\n",
    "\n",
    "Land cover classification from satellite imagery represents a critical downstream task for evaluating image restoration models. This task serves as an excellent testbed for restoration quality assessment because:\n",
    "\n",
    "1. **Real-world Impact**: Satellite-based land cover classification directly supports environmental monitoring, urban planning, and climate research applications\n",
    "2. **Multi-spectral Complexity**: Sentinel-2 satellite data contains 13 spectral bands, making it sensitive to restoration artifacts across different wavelengths\n",
    "3. **Fine-grained Classification**: With 19 distinct land cover classes, the task requires preservation of subtle spectral signatures that could be lost during restoration\n",
    "4. **Scale Sensitivity**: Classification performance depends on both local texture details and broader spatial patterns, testing restoration at multiple scales\n",
    "\n",
    "This evaluation framework provides comprehensive tools for assessing how well restored satellite imagery maintains the critical spectral and spatial information needed for accurate land cover classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b23ee",
   "metadata": {},
   "source": [
    "## Dataset and Model Information\n",
    "\n",
    "This evaluation uses Sentinel-2 satellite images covering 19 land cover classes based on the CORINE Land Cover database:\n",
    "\n",
    "**Class Labels:**\n",
    "1. Urban fabric\n",
    "2. Industrial or commercial units  \n",
    "3. Arable land\n",
    "4. Permanent crops\n",
    "5. Pastures\n",
    "6. Complex cultivation patterns\n",
    "7. Land principally occupied by agriculture\n",
    "8. Agro-forestry areas\n",
    "9. Broad-leaved forest\n",
    "10. Coniferous forest\n",
    "11. Mixed forest\n",
    "12. Natural grassland\n",
    "13. Moors and heathland\n",
    "14. Sclerophyllous vegetation\n",
    "15. Transitional woodland/shrub\n",
    "16. Beaches, dunes, sands\n",
    "17. Inland waters\n",
    "18. Coastal lagoons\n",
    "19. Estuaries\n",
    "\n",
    "### Required Files and Dependencies\n",
    "\n",
    "**Model Weights:**\n",
    "- Pre-trained models available on [Hugging Face Hub](https://huggingface.co/models?search=bigearthnet)\n",
    "- Official BigEarthNet models: [BigEarthNet website](http://bigearth.net/)\n",
    "- Supported formats: `.safetensors`, `.pth`, TensorFlow checkpoints\n",
    "\n",
    "**Dataset Access:**\n",
    "- BigEarthNet dataset: Available through official channels or processed versions on Hugging Face\n",
    "- Requires registration for full dataset access\n",
    "\n",
    "**Key Citations:**\n",
    "- Sumbul, G., et al. (2019). \"BigEarthNet: A Large-Scale Benchmark Archive for Remote Sensing Image Understanding.\" IGARSS 2019.\n",
    "- Sumbul, G., et al. (2021). \"BigEarthNet-MM: A Large-Scale, Multimodal, Multilabel Benchmark Archive for Remote Sensing Image Classification and Retrieval.\" IEEE Geoscience and Remote Sensing Magazine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517de6e",
   "metadata": {},
   "source": [
    "## Installation and Setup\n",
    "\n",
    "This section handles the installation of all required dependencies for the satellite image evaluation pipeline. The setup includes deep learning frameworks (PyTorch), image processing libraries (PIL, OpenCV), scientific computing tools (NumPy, Pandas), visualization packages (Matplotlib, Seaborn), and evaluation metrics utilities (scikit-learn). \n",
    "\n",
    "The installation process automatically detects missing packages and installs them, ensuring a complete environment for running land cover classification evaluations on satellite imagery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faae60e",
   "metadata": {},
   "source": [
    "## Batch Evaluation Framework\n",
    "\n",
    "This section provides the core functionality for running evaluation over batches of satellite images, saving outputs, and computing comprehensive evaluation metrics. The framework supports both single-image and batch processing modes with detailed performance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages_to_install = [\n",
    "    'torch',\n",
    "    'torchvision', \n",
    "    'safetensors',\n",
    "    'pillow',\n",
    "    'numpy',\n",
    "    'pandas',\n",
    "    'matplotlib',\n",
    "    'seaborn',\n",
    "    'scikit-learn',\n",
    "    'tqdm'\n",
    "]\n",
    "\n",
    "for package in packages_to_install:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "        print(f\"{package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "\n",
    "print(\"\\n All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccbd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from safetensors.torch import load_file\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c3f0e",
   "metadata": {},
   "source": [
    "## Class Definitions and Configuration\n",
    "\n",
    "This section defines the 19 land cover classes from the BigEarthNet dataset based on the CORINE Land Cover classification system. These classes represent the complete range of land cover types found in European satellite imagery, from urban environments to natural ecosystems.\n",
    "\n",
    "The configuration parameters are optimized for multi-spectral satellite image processing, including input dimensions that accommodate both RGB and 10-channel Sentinel-2 imagery. The batch size and processing parameters are set to balance computational efficiency with memory constraints during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definitions\n",
    "CLASSES = [\n",
    "    \"Urban fabric\",\n",
    "    \"Industrial or commercial units\", \n",
    "    \"Arable land\",\n",
    "    \"Permanent crops\",\n",
    "    \"Pastures\",\n",
    "    \"Complex cultivation patterns\",\n",
    "    \"Land principally occupied by agriculture, with significant areas of natural vegetation\",\n",
    "    \"Agro-forestry areas\",\n",
    "    \"Broad-leaved forest\",\n",
    "    \"Coniferous forest\", \n",
    "    \"Mixed forest\",\n",
    "    \"Natural grassland\",\n",
    "    \"Moors and heathland\",\n",
    "    \"Sclerophyllous vegetation\",\n",
    "    \"Transitional woodland/shrub\",\n",
    "    \"Beaches, dunes, sands\", \n",
    "    \"Inland waters\",\n",
    "    \"Coastal lagoons\",\n",
    "    \"Estuaries\"\n",
    "]\n",
    "\n",
    "# Class ID mapping\n",
    "CLASS_ID_TO_NAME = {i: name for i, name in enumerate(CLASSES)}\n",
    "CLASS_NAME_TO_ID = {name: i for i, name in enumerate(CLASSES)}\n",
    "\n",
    "print(f\"Number of classes: {len(CLASSES)}\")\n",
    "print(\"Class definitions loaded\")\n",
    "\n",
    "# Configuration for different evaluation scenarios\n",
    "EVAL_CONFIG = {\n",
    "    'input_size': (224, 224),\n",
    "    'input_channels': 10,  # Multi-spectral channels\n",
    "    'batch_size': 32,\n",
    "    'num_workers': 4,\n",
    "    'confidence_threshold': 0.5,\n",
    "    'supported_formats': ['.jpg', '.jpeg', '.png', '.tif', '.tiff'],\n",
    "    'model_architectures': ['resnet50', 'resnet101', 'wide_resnet50_2'],\n",
    "    'checkpoint_formats': ['.safetensors', '.pth', '.pt']\n",
    "}\n",
    "\n",
    "print(\"Evaluation configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3bf61b",
   "metadata": {},
   "source": [
    "## Model Loading and Preprocessing Pipeline\n",
    "\n",
    "This section implements a comprehensive preprocessing pipeline specifically designed for satellite imagery evaluation. The pipeline handles the complexities of multi-spectral satellite data while maintaining compatibility with standard RGB inputs.\n",
    "\n",
    "**Key Features:**\n",
    "- **Multi-spectral Support**: Converts RGB imagery to simulate 10-channel Sentinel-2 data by intelligently duplicating and modifying spectral bands\n",
    "- **Flexible Architecture Loading**: Supports multiple CNN architectures (ResNet-50, ResNet-101, Wide ResNet-50) commonly used in remote sensing\n",
    "- **Checkpoint Compatibility**: Handles various model checkpoint formats including SafeTensors and PyTorch formats\n",
    "- **Adaptive Input Layers**: Automatically modifies model input layers to accommodate multi-spectral channels\n",
    "\n",
    "The preprocessing strategy approximates additional spectral bands (NIR, SWIR, vegetation red edge) by applying mathematical transformations to RGB channels, enabling evaluation of models trained on full multi-spectral data using standard RGB imagery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8b561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteImageProcessor:\n",
    "    \"\"\"\n",
    "    Advanced preprocessing pipeline for satellite imagery\n",
    "    Supports multi-spectral (10-channel) and RGB (3-channel) inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=10, target_size=(224, 224)):\n",
    "        self.input_channels = input_channels\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Multi-spectral preprocessing (expands RGB to 10 channels)\n",
    "        self.multispectral_transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        # Standard RGB preprocessing\n",
    "        self.rgb_transform = transforms.Compose([\n",
    "            transforms.Resize(target_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def expand_rgb_to_multispectral(self, rgb_tensor):\n",
    "        \"\"\"Expand 3-channel RGB to 10-channel multi-spectral\"\"\"\n",
    "        if rgb_tensor.shape[0] != 3:\n",
    "            raise ValueError(f\"Expected 3-channel input, got {rgb_tensor.shape[0]}\")\n",
    "        \n",
    "        # Strategy: Duplicate RGB channels with slight variations\n",
    "        # This simulates multi-spectral data for evaluation purposes\n",
    "        channels = []\n",
    "        \n",
    "        # Original RGB channels\n",
    "        channels.extend([rgb_tensor[0], rgb_tensor[1], rgb_tensor[2]])\n",
    "        \n",
    "        # Near-infrared approximations (modified red channel)\n",
    "        channels.append(rgb_tensor[0] * 1.1)  # NIR1\n",
    "        channels.append(rgb_tensor[0] * 0.9)  # NIR2\n",
    "        \n",
    "        # SWIR approximations (modified combinations)\n",
    "        channels.append((rgb_tensor[0] + rgb_tensor[2]) * 0.5)  # SWIR1\n",
    "        channels.append((rgb_tensor[1] + rgb_tensor[2]) * 0.6)  # SWIR2\n",
    "        \n",
    "        # Vegetation red edge (modified red/green)\n",
    "        channels.append((rgb_tensor[0] + rgb_tensor[1]) * 0.7)  # VRE1\n",
    "        channels.append((rgb_tensor[0] + rgb_tensor[1]) * 0.8)  # VRE2\n",
    "        \n",
    "        # Coastal aerosol (modified blue)\n",
    "        channels.append(rgb_tensor[2] * 0.95)  # Coastal\n",
    "        \n",
    "        return torch.stack(channels)\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Process a single image\"\"\"\n",
    "        try:\n",
    "            # Load image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Apply basic preprocessing\n",
    "            tensor = self.rgb_transform(image)\n",
    "            \n",
    "            # Expand to multi-spectral if needed\n",
    "            if self.input_channels == 10:\n",
    "                tensor = self.expand_rgb_to_multispectral(tensor)\n",
    "            \n",
    "            return tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "def load_satellite_model(checkpoint_path, architecture='resnet50', num_classes=19, input_channels=10):\n",
    "    \"\"\"\n",
    "    Load a satellite image classification model from various checkpoint formats\n",
    "    \"\"\"\n",
    "    print(f\"Loading {architecture} model from {checkpoint_path}\")\n",
    "    \n",
    "    # Initialize model architecture\n",
    "    if architecture == 'resnet50':\n",
    "        model = models.resnet50(pretrained=False)\n",
    "    elif architecture == 'resnet101':\n",
    "        model = models.resnet101(pretrained=False)\n",
    "    elif architecture == 'wide_resnet50_2':\n",
    "        model = models.wide_resnet50_2(pretrained=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported architecture: {architecture}\")\n",
    "    \n",
    "    # Modify first layer for multi-spectral input\n",
    "    if input_channels != 3:\n",
    "        original_conv1 = model.conv1\n",
    "        model.conv1 = nn.Conv2d(\n",
    "            input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "        \n",
    "        # Initialize new conv1 weights\n",
    "        with torch.no_grad():\n",
    "            if input_channels == 10:\n",
    "                # For 10-channel input, initialize with RGB weights + noise\n",
    "                model.conv1.weight[:, :3, :, :] = original_conv1.weight\n",
    "                # Initialize additional channels with small random values\n",
    "                model.conv1.weight[:, 3:, :, :] = torch.randn(64, 7, 7, 7) * 0.01\n",
    "    \n",
    "    # Modify final layer for correct number of classes\n",
    "    if hasattr(model, 'fc'):\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif hasattr(model, 'classifier'):\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    \n",
    "    # Load weights\n",
    "    try:\n",
    "        if checkpoint_path.endswith('.safetensors'):\n",
    "            weights = load_file(checkpoint_path)\n",
    "            model.load_state_dict(weights, strict=False)\n",
    "            print(\"Loaded weights from .safetensors file\")\n",
    "        elif checkpoint_path.endswith(('.pth', '.pt')):\n",
    "            weights = torch.load(checkpoint_path, map_location='cpu')\n",
    "            if 'state_dict' in weights:\n",
    "                weights = weights['state_dict']\n",
    "            model.load_state_dict(weights, strict=False)\n",
    "            print(\"Loaded weights from PyTorch checkpoint\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported checkpoint format: {checkpoint_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load all weights - {e}\")\n",
    "        print(\"Proceeding with randomly initialized weights for missing parameters\")\n",
    "    \n",
    "    model.eval()\n",
    "    return model.to(device)\n",
    "\n",
    "# Initialize processor\n",
    "processor = SatelliteImageProcessor(\n",
    "    input_channels=EVAL_CONFIG['input_channels'],\n",
    "    target_size=EVAL_CONFIG['input_size']\n",
    ")\n",
    "\n",
    "print(\"Model loading utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bb47d",
   "metadata": {},
   "source": [
    "## Batch Inference Pipeline\n",
    "\n",
    "This section implements an efficient batch processing system for running land cover classification on large datasets of satellite images. The pipeline is designed to handle both single images and batch processing scenarios while maintaining consistent preprocessing and evaluation standards.\n",
    "\n",
    "**Pipeline Components:**\n",
    "- **Batch Loading**: Efficient data loading with configurable batch sizes and parallel processing\n",
    "- **Memory Management**: Optimized memory usage for processing large satellite image datasets\n",
    "- **Progress Tracking**: Real-time progress monitoring with detailed timing information\n",
    "- **Error Handling**: Robust error handling for corrupted or incompatible image formats\n",
    "- **Output Management**: Structured output format for downstream analysis and metric computation\n",
    "\n",
    "The batch inference system supports various image formats commonly used in satellite imagery (TIFF, JPEG, PNG) and automatically handles preprocessing, inference, and result aggregation across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250ecaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(results, output_file):\n",
    "    \"\"\"Save inference results to CSV format\"\"\"\n",
    "    \n",
    "    rows = []\n",
    "    for img_name, prediction in results['predictions'].items():\n",
    "        if 'error' not in prediction:\n",
    "            rows.append({\n",
    "                'image_name': img_name,\n",
    "                'predicted_class': prediction['predicted_class'],\n",
    "                'predicted_class_id': prediction['predicted_class_id'],\n",
    "                'confidence_score': prediction['confidence_score'],\n",
    "                'inference_time': prediction.get('inference_time', 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "def run_batch_inference(model, input_folder, output_file=None, max_images=None):\n",
    "    \"\"\"\n",
    "    Run batch inference on a folder of satellite images\n",
    "    \n",
    "    Args:\n",
    "        model: Loaded PyTorch model\n",
    "        input_folder: Path to folder containing images\n",
    "        output_file: Optional path to save results CSV\n",
    "        max_images: Optional limit on number of images to process\n",
    "    \n",
    "    Returns:\n",
    "        dict: Comprehensive results dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting batch inference on: {input_folder}\")\n",
    "    \n",
    "    # Find all image files\n",
    "    supported_extensions = EVAL_CONFIG['supported_formats']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in supported_extensions:\n",
    "        image_files.extend([\n",
    "            f for f in os.listdir(input_folder) \n",
    "            if f.lower().endswith(ext.lower())\n",
    "        ])\n",
    "    \n",
    "    if max_images:\n",
    "        image_files = image_files[:max_images]\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images to process\")\n",
    "    \n",
    "    if len(image_files) == 0:\n",
    "        print(\"No supported image files found!\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize results tracking\n",
    "    results = {\n",
    "        'predictions': {},\n",
    "        'metadata': {\n",
    "            'model_name': model.__class__.__name__,\n",
    "            'num_classes': len(CLASSES),\n",
    "            'class_names': CLASSES,\n",
    "            'input_resolution': EVAL_CONFIG['input_size'],\n",
    "            'input_channels': EVAL_CONFIG['input_channels'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'input_folder': input_folder\n",
    "        },\n",
    "        'performance': {\n",
    "            'total_images': len(image_files),\n",
    "            'successful_predictions': 0,\n",
    "            'failed_predictions': 0,\n",
    "            'inference_times': [],\n",
    "            'total_processing_time': 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process images with progress bar\n",
    "    for img_name in tqdm(image_files, desc=\"Processing images\"):\n",
    "        img_path = os.path.join(input_folder, img_name)\n",
    "        \n",
    "        try:\n",
    "            # Start timing\n",
    "            inference_start = time.time()\n",
    "            \n",
    "            # Process image\n",
    "            input_tensor = processor.process_image(img_path)\n",
    "            if input_tensor is None:\n",
    "                results['performance']['failed_predictions'] += 1\n",
    "                continue\n",
    "                \n",
    "            # Add batch dimension and move to device\n",
    "            input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "            \n",
    "            # Run inference\n",
    "            with torch.no_grad():\n",
    "                raw_output = model(input_batch)\n",
    "                probabilities = F.softmax(raw_output, dim=1)\n",
    "                confidence, predicted = torch.max(probabilities, 1)\n",
    "                \n",
    "                # Extract results\n",
    "                pred_idx = predicted.item()\n",
    "                confidence_score = confidence.item()\n",
    "                raw_logits = raw_output.squeeze().cpu().numpy().tolist()\n",
    "                probs = probabilities.squeeze().cpu().numpy().tolist()\n",
    "            \n",
    "            # Record timing\n",
    "            inference_time = time.time() - inference_start\n",
    "            results['performance']['inference_times'].append(inference_time)\n",
    "            \n",
    "            # Store prediction results\n",
    "            results['predictions'][img_name] = {\n",
    "                'predicted_class': CLASSES[pred_idx],\n",
    "                'predicted_class_id': pred_idx,\n",
    "                'confidence_score': confidence_score,\n",
    "                'raw_logits': raw_logits,\n",
    "                'probabilities': probs,\n",
    "                'inference_time': inference_time\n",
    "            }\n",
    "            \n",
    "            results['performance']['successful_predictions'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_name}: {e}\")\n",
    "            results['predictions'][img_name] = {\n",
    "                'error': str(e),\n",
    "                'predicted_class': 'ERROR',\n",
    "                'predicted_class_id': -1,\n",
    "                'confidence_score': 0.0\n",
    "            }\n",
    "            results['performance']['failed_predictions'] += 1\n",
    "    \n",
    "    # Calculate final performance metrics\n",
    "    total_time = time.time() - start_time\n",
    "    results['performance']['total_processing_time'] = total_time\n",
    "    results['performance']['average_inference_time'] = (\n",
    "        np.mean(results['performance']['inference_times']) \n",
    "        if results['performance']['inference_times'] else 0\n",
    "    )\n",
    "    results['performance']['images_per_second'] = (\n",
    "        results['performance']['successful_predictions'] / total_time\n",
    "        if total_time > 0 else 0\n",
    "    )\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"BATCH INFERENCE COMPLETE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total images: {results['performance']['total_images']}\")\n",
    "    print(f\"Successful: {results['performance']['successful_predictions']}\")\n",
    "    print(f\"Failed: {results['performance']['failed_predictions']}\")\n",
    "    print(f\"Success rate: {results['performance']['successful_predictions']/results['performance']['total_images']*100:.1f}%\")\n",
    "    print(f\"Average inference time: {results['performance']['average_inference_time']:.3f}s\")\n",
    "    print(f\"Processing speed: {results['performance']['images_per_second']:.1f} images/sec\")\n",
    "    print(f\"Total time: {total_time:.1f}s\")\n",
    "    \n",
    "    # Save results to CSV if requested\n",
    "    if output_file:\n",
    "        save_results_to_csv(results, output_file)\n",
    "        print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    return results\n",
    "    \n",
    "print(\"Batch inference pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01442d78",
   "metadata": {},
   "source": [
    "## Evaluation Metrics and Classification Assessment\n",
    "\n",
    "This section implements a comprehensive evaluation framework for assessing land cover classification performance. The evaluation system computes multiple complementary metrics to provide a thorough understanding of model performance across different aspects of the classification task.\n",
    "\n",
    "### Primary Evaluation Metrics\n",
    "\n",
    "**Accuracy Metrics:**\n",
    "- **Overall Accuracy**: The fraction of correctly classified pixels/images across all classes\n",
    "- **Top-k Accuracy**: Measures if the correct class appears in the top-k predictions (k=3,5), useful for understanding near-miss predictions\n",
    "- **Per-class Accuracy**: Individual accuracy scores for each of the 19 land cover classes\n",
    "\n",
    "**Precision, Recall, and F1-Score:**\n",
    "- **Precision**: For each class, the fraction of predicted instances that are actually correct (TP/(TP+FP))\n",
    "- **Recall (Sensitivity)**: For each class, the fraction of actual instances that are correctly identified (TP/(TP+FN))\n",
    "- **F1-Score**: Harmonic mean of precision and recall, providing a balanced measure of performance\n",
    "- **Macro-averaged**: Unweighted mean across all classes, treating each class equally\n",
    "- **Weighted-averaged**: Mean weighted by class support, accounting for class imbalance\n",
    "\n",
    "**Confusion Matrix Analysis:**\n",
    "- **Raw Confusion Matrix**: Shows the distribution of predictions vs ground truth for detailed error analysis\n",
    "- **Normalized Confusion Matrix**: Percentages normalized by true class, highlighting misclassification patterns\n",
    "- **Class Confusion Patterns**: Identifies which classes are commonly confused with each other\n",
    "\n",
    "**Confidence Analysis:**\n",
    "- **Prediction Confidence**: Distribution of model confidence scores across all predictions\n",
    "- **Confidence by Correctness**: Comparison of confidence scores between correct and incorrect predictions\n",
    "- **Confidence Calibration**: Assessment of whether high confidence correlates with correct predictions\n",
    "\n",
    "### Advanced Evaluation Features\n",
    "\n",
    "**Class Distribution Analysis:**\n",
    "- Compares the distribution of true labels vs predicted labels to identify prediction biases\n",
    "- Identifies under-predicted and over-predicted classes\n",
    "\n",
    "**Error Pattern Analysis:**\n",
    "- Systematic analysis of common misclassification patterns\n",
    "- Identifies challenging class pairs and potential systematic errors\n",
    "\n",
    "The evaluation framework is designed to handle class imbalance common in satellite imagery datasets and provides insights for model improvement and deployment decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c17106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation metrics for land cover classification\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, class_names=None):\n",
    "        self.class_names = class_names or CLASSES\n",
    "        self.num_classes = len(self.class_names)\n",
    "    \n",
    "    def load_ground_truth(self, ground_truth_file):\n",
    "        \"\"\"\n",
    "        Load ground truth labels from various formats\n",
    "        \n",
    "        Supported formats:\n",
    "        - CSV with columns: image_name, true_class_id or true_class\n",
    "        - JSON with image_name -> class mapping\n",
    "        - TXT with format: image_name: class_id (one per line)\n",
    "        \"\"\"\n",
    "        \n",
    "        if ground_truth_file.endswith('.csv'):\n",
    "            df = pd.read_csv(ground_truth_file)\n",
    "            ground_truth = {}\n",
    "            \n",
    "            if 'true_class_id' in df.columns:\n",
    "                for _, row in df.iterrows():\n",
    "                    ground_truth[row['image_name']] = {\n",
    "                        'true_class_id': int(row['true_class_id']),\n",
    "                        'true_class': self.class_names[int(row['true_class_id'])]\n",
    "                    }\n",
    "            elif 'true_class' in df.columns:\n",
    "                for _, row in df.iterrows():\n",
    "                    class_id = CLASS_NAME_TO_ID.get(row['true_class'], -1)\n",
    "                    ground_truth[row['image_name']] = {\n",
    "                        'true_class_id': class_id,\n",
    "                        'true_class': row['true_class']\n",
    "                    }\n",
    "            else:\n",
    "                raise ValueError(\"CSV must contain 'true_class_id' or 'true_class' column\")\n",
    "                \n",
    "        elif ground_truth_file.endswith('.json'):\n",
    "            with open(ground_truth_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            ground_truth = {}\n",
    "            \n",
    "            for img_name, label_info in data.items():\n",
    "                if isinstance(label_info, dict):\n",
    "                    ground_truth[img_name] = label_info\n",
    "                else:\n",
    "                    # Assume it's just the class name or ID\n",
    "                    if isinstance(label_info, str):\n",
    "                        class_id = CLASS_NAME_TO_ID.get(label_info, -1)\n",
    "                        ground_truth[img_name] = {\n",
    "                            'true_class_id': class_id,\n",
    "                            'true_class': label_info\n",
    "                        }\n",
    "                    else:\n",
    "                        ground_truth[img_name] = {\n",
    "                            'true_class_id': int(label_info),\n",
    "                            'true_class': self.class_names[int(label_info)]\n",
    "                        }\n",
    "                        \n",
    "        elif ground_truth_file.endswith('.txt'):\n",
    "            ground_truth = {}\n",
    "            \n",
    "            with open(ground_truth_file, 'r') as f:\n",
    "                for line_num, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if not line:  # Skip empty lines\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Parse format: \"image_name: class_id\"\n",
    "                        if ':' not in line:\n",
    "                            print(f\"Warning: Line {line_num} missing colon separator: {line}\")\n",
    "                            continue\n",
    "                            \n",
    "                        img_name, class_id_str = line.split(':', 1)\n",
    "                        img_name = img_name.strip()\n",
    "                        class_id_str = class_id_str.strip()\n",
    "                        \n",
    "                        # Convert class_id to integer\n",
    "                        class_id = int(class_id_str)\n",
    "                        \n",
    "                        # Validate class_id is within valid range\n",
    "                        if 0 <= class_id < len(self.class_names):\n",
    "                            ground_truth[img_name] = {\n",
    "                                'true_class_id': class_id,\n",
    "                                'true_class': self.class_names[class_id]\n",
    "                            }\n",
    "                        else:\n",
    "                            print(f\"Warning: Line {line_num} has invalid class_id {class_id} (must be 0-{len(self.class_names)-1}): {line}\")\n",
    "                            \n",
    "                    except ValueError as e:\n",
    "                        print(f\"Warning: Line {line_num} has invalid format: {line} - {e}\")\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Error parsing line {line_num}: {line} - {e}\")\n",
    "                        continue\n",
    "            \n",
    "            print(f\"Loaded {len(ground_truth)} ground truth entries from TXT file\")\n",
    "                        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ground truth format. Use .csv, .json, or .txt\")\n",
    "        \n",
    "        return ground_truth\n",
    "    \n",
    "    def compute_metrics(self, predictions, ground_truth):\n",
    "        \"\"\"\n",
    "        Compute comprehensive classification metrics\n",
    "        \n",
    "        Args:\n",
    "            predictions: Dictionary from run_batch_inference\n",
    "            ground_truth: Dictionary with true labels\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with all computed metrics\n",
    "        \"\"\"\n",
    "        \n",
    "        # Align predictions and ground truth\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_confidence = []\n",
    "        matched_images = []\n",
    "        \n",
    "        for img_name in predictions['predictions']:\n",
    "            if img_name in ground_truth and 'error' not in predictions['predictions'][img_name]:\n",
    "                true_class_id = ground_truth[img_name]['true_class_id']\n",
    "                pred_class_id = predictions['predictions'][img_name]['predicted_class_id']\n",
    "                confidence = predictions['predictions'][img_name]['confidence_score']\n",
    "                \n",
    "                if true_class_id >= 0 and pred_class_id >= 0:  # Valid labels\n",
    "                    y_true.append(true_class_id)\n",
    "                    y_pred.append(pred_class_id)\n",
    "                    y_confidence.append(confidence)\n",
    "                    matched_images.append(img_name)\n",
    "        \n",
    "        if len(y_true) == 0:\n",
    "            raise ValueError(\"No matching images found between predictions and ground truth\")\n",
    "        \n",
    "        print(f\"Evaluating {len(y_true)} images with ground truth labels\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_confidence = np.array(y_confidence)\n",
    "        \n",
    "        # Compute basic metrics\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average=None, zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Macro averages\n",
    "        macro_precision = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='macro', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Weighted averages\n",
    "        weighted_precision = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='weighted', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=range(self.num_classes))\n",
    "        \n",
    "        # Get unique classes present in the data\n",
    "        unique_classes = sorted(list(set(y_true) | set(y_pred)))\n",
    "        present_class_names = [self.class_names[i] for i in unique_classes if i < len(self.class_names)]\n",
    "        \n",
    "        # Classification report (only for classes present in data)\n",
    "        class_report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            labels=unique_classes,\n",
    "            target_names=present_class_names,\n",
    "            zero_division=0,\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Top-k accuracy (top-3, top-5)\n",
    "        top3_acc = self.compute_topk_accuracy(predictions, ground_truth, k=3)\n",
    "        top5_acc = self.compute_topk_accuracy(predictions, ground_truth, k=5)\n",
    "        \n",
    "        # Class distribution analysis\n",
    "        class_distribution = {\n",
    "            'true_distribution': np.bincount(y_true, minlength=self.num_classes),\n",
    "            'pred_distribution': np.bincount(y_pred, minlength=self.num_classes)\n",
    "        }\n",
    "        \n",
    "        # Confidence analysis\n",
    "        confidence_stats = {\n",
    "            'mean_confidence': float(np.mean(y_confidence)),\n",
    "            'std_confidence': float(np.std(y_confidence)),\n",
    "            'min_confidence': float(np.min(y_confidence)),\n",
    "            'max_confidence': float(np.max(y_confidence)),\n",
    "            'median_confidence': float(np.median(y_confidence))\n",
    "        }\n",
    "        \n",
    "        # Confidence by correctness\n",
    "        correct_mask = (y_true == y_pred)\n",
    "        confidence_by_correctness = {\n",
    "            'correct_predictions': {\n",
    "                'mean_confidence': float(np.mean(y_confidence[correct_mask])) if np.any(correct_mask) else 0,\n",
    "                'count': int(np.sum(correct_mask))\n",
    "            },\n",
    "            'incorrect_predictions': {\n",
    "                'mean_confidence': float(np.mean(y_confidence[~correct_mask])) if np.any(~correct_mask) else 0,\n",
    "                'count': int(np.sum(~correct_mask))\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Compile all metrics\n",
    "        metrics = {\n",
    "            'overall_metrics': {\n",
    "                'accuracy': float(accuracy),\n",
    "                'top3_accuracy': float(top3_acc),\n",
    "                'top5_accuracy': float(top5_acc),\n",
    "                'macro_precision': float(macro_precision[0]),\n",
    "                'macro_recall': float(macro_precision[1]),\n",
    "                'macro_f1': float(macro_precision[2]),\n",
    "                'weighted_precision': float(weighted_precision[0]),\n",
    "                'weighted_recall': float(weighted_precision[1]),\n",
    "                'weighted_f1': float(weighted_precision[2]),\n",
    "                'num_samples': len(y_true)\n",
    "            },\n",
    "            'per_class_metrics': {\n",
    "                self.class_names[i]: {\n",
    "                    'precision': float(precision[i]) if i < len(precision) else 0.0,\n",
    "                    'recall': float(recall[i]) if i < len(recall) else 0.0,\n",
    "                    'f1_score': float(f1[i]) if i < len(f1) else 0.0,\n",
    "                    'support': int(support[i]) if i < len(support) else 0\n",
    "                }\n",
    "                for i in range(self.num_classes)\n",
    "            },\n",
    "            'confusion_matrix': cm.tolist(),\n",
    "            'classification_report': class_report,\n",
    "            'class_distribution': {\n",
    "                'true_distribution': class_distribution['true_distribution'].tolist(),\n",
    "                'pred_distribution': class_distribution['pred_distribution'].tolist()\n",
    "            },\n",
    "            'confidence_analysis': confidence_stats,\n",
    "            'confidence_by_correctness': confidence_by_correctness,\n",
    "            'matched_images': matched_images\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def compute_topk_accuracy(self, predictions, ground_truth, k=3):\n",
    "        \"\"\"Compute top-k accuracy\"\"\"\n",
    "        \n",
    "        correct_topk = 0\n",
    "        total = 0\n",
    "        \n",
    "        for img_name in predictions['predictions']:\n",
    "            if img_name in ground_truth and 'error' not in predictions['predictions'][img_name]:\n",
    "                true_class_id = ground_truth[img_name]['true_class_id']\n",
    "                probs = predictions['predictions'][img_name]['probabilities']\n",
    "                \n",
    "                if true_class_id >= 0 and len(probs) >= k:\n",
    "                    # Get top-k predictions\n",
    "                    top_k_indices = np.argsort(probs)[-k:]\n",
    "                    if true_class_id in top_k_indices:\n",
    "                        correct_topk += 1\n",
    "                    total += 1\n",
    "        \n",
    "        return correct_topk / total if total > 0 else 0\n",
    "    \n",
    "    def print_evaluation_summary(self, metrics):\n",
    "        \"\"\"Print a comprehensive evaluation summary\"\"\"\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"CLASSIFICATION EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall = metrics['overall_metrics']\n",
    "        print(f\"\\nOVERALL PERFORMANCE:\")\n",
    "        print(f\"  • Accuracy: {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)\")\n",
    "        print(f\"  • Top-3 Accuracy: {overall['top3_accuracy']:.4f} ({overall['top3_accuracy']*100:.2f}%)\")\n",
    "        print(f\"  • Top-5 Accuracy: {overall['top5_accuracy']:.4f} ({overall['top5_accuracy']*100:.2f}%)\")\n",
    "        print(f\"  • Macro F1-Score: {overall['macro_f1']:.4f}\")\n",
    "        print(f\"  • Weighted F1-Score: {overall['weighted_f1']:.4f}\")\n",
    "        print(f\"  • Number of samples: {overall['num_samples']}\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        conf = metrics['confidence_analysis']\n",
    "        print(f\"\\nCONFIDENCE ANALYSIS:\")\n",
    "        print(f\"  • Mean confidence: {conf['mean_confidence']:.4f}\")\n",
    "        print(f\"  • Confidence range: [{conf['min_confidence']:.4f}, {conf['max_confidence']:.4f}]\")\n",
    "        \n",
    "        conf_correct = metrics['confidence_by_correctness']\n",
    "        print(f\"  • Correct predictions: {conf_correct['correct_predictions']['mean_confidence']:.4f} (n={conf_correct['correct_predictions']['count']})\")\n",
    "        print(f\"  • Incorrect predictions: {conf_correct['incorrect_predictions']['mean_confidence']:.4f} (n={conf_correct['incorrect_predictions']['count']})\")\n",
    "        \n",
    "        # Top and bottom performing classes\n",
    "        per_class = metrics['per_class_metrics']\n",
    "        f1_scores = [(name, data['f1_score']) for name, data in per_class.items() if data['support'] > 0]\n",
    "        f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(f\"\\nTOP 5 PERFORMING CLASSES (by F1-score):\")\n",
    "        for i, (class_name, f1) in enumerate(f1_scores[:5]):\n",
    "            support = per_class[class_name]['support']\n",
    "            print(f\"  {i+1}. {class_name}: {f1:.4f} (n={support})\")\n",
    "        \n",
    "        print(f\"\\nBOTTOM 5 PERFORMING CLASSES (by F1-score):\")\n",
    "        for i, (class_name, f1) in enumerate(f1_scores[-5:]):\n",
    "            support = per_class[class_name]['support']\n",
    "            print(f\"  {len(f1_scores)-i}. {class_name}: {f1:.4f} (n={support})\")\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = ClassificationEvaluator(CLASSES)\n",
    "print(\"Classification evaluator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b98072",
   "metadata": {},
   "source": [
    "## Visualization and Analysis Tools\n",
    "\n",
    "This section provides comprehensive visualization capabilities for analyzing classification performance and understanding model behavior. The visualization suite includes confusion matrices, performance distributions, confidence analysis plots, and detailed error analysis visualizations.\n",
    "\n",
    "**Visualization Components:**\n",
    "- **Confusion Matrix Heatmaps**: Normalized confusion matrices with class-wise performance visualization\n",
    "- **Per-Class Performance Charts**: Bar charts showing precision, recall, and F1-scores for each land cover class\n",
    "- **Confidence Distribution Plots**: Histograms and box plots showing confidence score distributions\n",
    "- **Class Distribution Comparisons**: Side-by-side comparison of true vs predicted class distributions\n",
    "- **Error Analysis Visualizations**: Detailed plots highlighting misclassification patterns and systematic errors\n",
    "\n",
    "These visualizations are essential for understanding model strengths and weaknesses, identifying problematic classes, and making informed decisions about model deployment and improvement strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7623d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(metrics, figsize=(12, 10), save_path=None):\n",
    "    \"\"\"Plot confusion matrix with proper labels\"\"\"\n",
    "    \n",
    "    cm = np.array(metrics['confusion_matrix'])\n",
    "    class_names = CLASSES\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)  # Handle divide by zero\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(cm_normalized, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                xticklabels=[name[:20] + '...' if len(name) > 20 else name for name in class_names],\n",
    "                yticklabels=[name[:20] + '...' if len(name) > 20 else name for name in class_names],\n",
    "                cbar_kws={'label': 'Normalized Frequency'})\n",
    "    \n",
    "    plt.title('Confusion Matrix - Land Cover Classification\\n(Normalized by True Class)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class', fontsize=12)\n",
    "    plt.ylabel('True Class', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_class_performance(metrics, figsize=(15, 8), save_path=None):\n",
    "    \"\"\"Plot per-class performance metrics\"\"\"\n",
    "    \n",
    "    per_class = metrics['per_class_metrics']\n",
    "    \n",
    "    # Extract data\n",
    "    classes = list(per_class.keys())\n",
    "    f1_scores = [per_class[cls]['f1_score'] for cls in classes]\n",
    "    precisions = [per_class[cls]['precision'] for cls in classes]\n",
    "    recalls = [per_class[cls]['recall'] for cls in classes]\n",
    "    supports = [per_class[cls]['support'] for cls in classes]\n",
    "    \n",
    "    # Create subplot\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=figsize)\n",
    "    \n",
    "    # Plot 1: F1-scores with support size\n",
    "    x_pos = np.arange(len(classes))\n",
    "    bars = ax1.bar(x_pos, f1_scores, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    \n",
    "    # Color bars by support size\n",
    "    max_support = max(supports) if supports else 1\n",
    "    for i, (bar, support) in enumerate(zip(bars, supports)):\n",
    "        color_intensity = support / max_support\n",
    "        bar.set_color(plt.cm.viridis(color_intensity))\n",
    "    \n",
    "    ax1.set_xlabel('Land Cover Classes')\n",
    "    ax1.set_ylabel('F1-Score')\n",
    "    ax1.set_title('Per-Class F1-Scores (Color intensity = Support size)', fontweight='bold')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels([cls[:15] + '...' if len(cls) > 15 else cls for cls in classes], \n",
    "                        rotation=45, ha='right')\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Plot 2: Precision vs Recall scatter\n",
    "    scatter = ax2.scatter(recalls, precisions, s=[s*2 for s in supports], \n",
    "                         alpha=0.6, c=supports, cmap='viridis')\n",
    "    \n",
    "    # Add class labels for points with good performance or high support\n",
    "    for i, cls in enumerate(classes):\n",
    "        if f1_scores[i] > 0.5 or supports[i] > np.percentile(supports, 75):\n",
    "            ax2.annotate(cls[:10], (recalls[i], precisions[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points', \n",
    "                        fontsize=8, alpha=0.7)\n",
    "    \n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision vs Recall (Bubble size = Support)', fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add diagonal line\n",
    "    ax2.plot([0, 1], [0, 1], 'k--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax2)\n",
    "    cbar.set_label('Support (Number of samples)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Class performance plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_confidence_analysis(metrics, figsize=(12, 8), save_path=None):\n",
    "    \"\"\"Plot confidence score analysis\"\"\"\n",
    "    \n",
    "    # This would need actual confidence data from predictions\n",
    "    # For now, create a placeholder visualization framework\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Confidence statistics from metrics\n",
    "    conf_stats = metrics['confidence_analysis']\n",
    "    conf_by_correct = metrics['confidence_by_correctness']\n",
    "    \n",
    "    # Plot 1: Basic confidence statistics\n",
    "    stats_names = ['Mean', 'Median', 'Min', 'Max']\n",
    "    stats_values = [\n",
    "        conf_stats['mean_confidence'],\n",
    "        conf_stats['median_confidence'], \n",
    "        conf_stats['min_confidence'],\n",
    "        conf_stats['max_confidence']\n",
    "    ]\n",
    "    \n",
    "    ax1.bar(stats_names, stats_values, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow'])\n",
    "    ax1.set_title('Confidence Score Statistics', fontweight='bold')\n",
    "    ax1.set_ylabel('Confidence Score')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Confidence by correctness\n",
    "    correct_conf = conf_by_correct['correct_predictions']['mean_confidence']\n",
    "    incorrect_conf = conf_by_correct['incorrect_predictions']['mean_confidence']\n",
    "    correct_count = conf_by_correct['correct_predictions']['count']\n",
    "    incorrect_count = conf_by_correct['incorrect_predictions']['count']\n",
    "    \n",
    "    bars = ax2.bar(['Correct\\nPredictions', 'Incorrect\\nPredictions'], \n",
    "                   [correct_conf, incorrect_conf], \n",
    "                   color=['green', 'red'], alpha=0.7)\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    ax2.text(0, correct_conf + 0.02, f'n={correct_count}', ha='center', fontweight='bold')\n",
    "    ax2.text(1, incorrect_conf + 0.02, f'n={incorrect_count}', ha='center', fontweight='bold')\n",
    "    \n",
    "    ax2.set_title('Mean Confidence by Prediction Correctness', fontweight='bold')\n",
    "    ax2.set_ylabel('Mean Confidence Score')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Class distribution comparison\n",
    "    true_dist = metrics['class_distribution']['true_distribution']\n",
    "    pred_dist = metrics['class_distribution']['pred_distribution']\n",
    "    \n",
    "    x_pos = np.arange(len(true_dist))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x_pos - width/2, true_dist, width, label='True Distribution', alpha=0.7)\n",
    "    ax3.bar(x_pos + width/2, pred_dist, width, label='Predicted Distribution', alpha=0.7)\n",
    "    \n",
    "    ax3.set_title('Class Distribution: True vs Predicted', fontweight='bold')\n",
    "    ax3.set_xlabel('Class Index')\n",
    "    ax3.set_ylabel('Count')\n",
    "    ax3.legend()\n",
    "    ax3.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Overall accuracy summary\n",
    "    overall = metrics['overall_metrics']\n",
    "    accuracy_metrics = {\n",
    "        'Top-1': overall['accuracy'],\n",
    "        'Top-3': overall['top3_accuracy'],\n",
    "        'Top-5': overall['top5_accuracy']\n",
    "    }\n",
    "    \n",
    "    bars = ax4.bar(accuracy_metrics.keys(), accuracy_metrics.values(), \n",
    "                   color=['darkblue', 'blue', 'lightblue'])\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for bar, acc in zip(bars, accuracy_metrics.values()):\n",
    "        height = bar.get_height()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc*100:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax4.set_title('Top-K Accuracy Comparison', fontweight='bold')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.set_ylim(0, 1)\n",
    "    ax4.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Confidence analysis plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def generate_evaluation_report(metrics, output_dir='evaluation_results'):\n",
    "    \"\"\"Generate a comprehensive evaluation report with visualizations\"\"\"\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Generating comprehensive evaluation report in: {output_dir}\")\n",
    "    \n",
    "    # Generate visualizations\n",
    "    plot_confusion_matrix(metrics, save_path=os.path.join(output_dir, 'confusion_matrix.png'))\n",
    "    plot_class_performance(metrics, save_path=os.path.join(output_dir, 'class_performance.png'))\n",
    "    plot_confidence_analysis(metrics, save_path=os.path.join(output_dir, 'confidence_analysis.png'))\n",
    "    \n",
    "    # Save detailed metrics as JSON\n",
    "    with open(os.path.join(output_dir, 'detailed_metrics.json'), 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Generate summary report\n",
    "    with open(os.path.join(output_dir, 'evaluation_summary.txt'), 'w') as f:\n",
    "        f.write(\"LAND COVER CLASSIFICATION EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(f\"Generated on: {datetime.now().isoformat()}\\n\\n\")\n",
    "        \n",
    "        # Overall metrics\n",
    "        overall = metrics['overall_metrics']\n",
    "        f.write(\"OVERALL PERFORMANCE:\\n\")\n",
    "        f.write(f\"  Accuracy: {overall['accuracy']:.4f} ({overall['accuracy']*100:.2f}%)\\n\")\n",
    "        f.write(f\"  Top-3 Accuracy: {overall['top3_accuracy']:.4f} ({overall['top3_accuracy']*100:.2f}%)\\n\")\n",
    "        f.write(f\"  Top-5 Accuracy: {overall['top5_accuracy']:.4f} ({overall['top5_accuracy']*100:.2f}%)\\n\")\n",
    "        f.write(f\"  Macro F1-Score: {overall['macro_f1']:.4f}\\n\")\n",
    "        f.write(f\"  Weighted F1-Score: {overall['weighted_f1']:.4f}\\n\")\n",
    "        f.write(f\"  Number of samples: {overall['num_samples']}\\n\\n\")\n",
    "        \n",
    "        # Per-class performance summary\n",
    "        per_class = metrics['per_class_metrics']\n",
    "        f1_scores = [(name, data['f1_score'], data['support']) for name, data in per_class.items()]\n",
    "        f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        f.write(\"TOP 10 PERFORMING CLASSES:\\n\")\n",
    "        for i, (class_name, f1, support) in enumerate(f1_scores[:10]):\n",
    "            f.write(f\"  {i+1:2d}. {class_name:<45} F1: {f1:.4f} (n={support})\\n\")\n",
    "        \n",
    "        f.write(\"\\nBOTTOM 10 PERFORMING CLASSES:\\n\")\n",
    "        for i, (class_name, f1, support) in enumerate(f1_scores[-10:]):\n",
    "            rank = len(f1_scores) - 9 + i\n",
    "            f.write(f\"  {rank:2d}. {class_name:<45} F1: {f1:.4f} (n={support})\\n\")\n",
    "    \n",
    "    print(f\"Evaluation report generated in: {output_dir}\")\n",
    "    print(\"  - confusion_matrix.png\")\n",
    "    print(\"  - class_performance.png\")\n",
    "    print(\"  - confidence_analysis.png\")\n",
    "    print(\"  - detailed_metrics.json\")\n",
    "    print(\"  - evaluation_summary.txt\")\n",
    "    print(\"Visualization and analysis tools ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdb7952",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline Demo\n",
    "\n",
    "This section provides a complete end-to-end example of running the satellite image land cover classification evaluation. The pipeline demonstrates the full workflow from model loading through batch inference to comprehensive evaluation and visualization.\n",
    "\n",
    "**Pipeline Steps:**\n",
    "1. **Model Loading**: Load pre-trained models with automatic architecture detection and weight loading\n",
    "2. **Batch Inference**: Process entire datasets with efficient batch processing and progress tracking\n",
    "3. **Ground Truth Alignment**: Match predictions with ground truth labels from various file formats\n",
    "4. **Metric Computation**: Calculate comprehensive evaluation metrics including accuracy, precision, recall, and confidence analysis\n",
    "5. **Visualization Generation**: Create detailed plots and visualizations for performance analysis\n",
    "6. **Report Generation**: Generate comprehensive evaluation reports with summaries and detailed results\n",
    "\n",
    "The example below shows how to adapt the pipeline for your specific model checkpoints, datasets, and evaluation requirements. Simply modify the file paths and configuration parameters to match your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE USAGE: Complete Evaluation Pipeline\n",
    "# Modify these paths to match your setup\n",
    "\n",
    "# Configuration\n",
    "MODEL_CHECKPOINT = \"model.safetensors\"  # Path to your model weights\n",
    "INPUT_FOLDER = \"test_images\"            # Folder containing test images  \n",
    "GROUND_TRUTH_FILE = \"Satellite_Downstream/ground_truth.txt\"  # Ground truth labels (optional)\n",
    "OUTPUT_DIR = \"evaluation_results\"       # Where to save results\n",
    "print(os.path.exists(\"Downloads/checkpoint_ResNet50/checkpoint_WideResNet_ECA\"))\n",
    "# Check if files exist\n",
    "files_exist = {\n",
    "    \"model\": os.path.exists(MODEL_CHECKPOINT),\n",
    "    \"input_folder\": os.path.exists(INPUT_FOLDER),\n",
    "    \"ground_truth\": os.path.exists(GROUND_TRUTH_FILE) if GROUND_TRUTH_FILE else False\n",
    "}\n",
    "\n",
    "print(\"FILE STATUS CHECK:\")\n",
    "print(f\"  Model checkpoint: {'FOUND' if files_exist['model'] else 'MISSING'} {MODEL_CHECKPOINT}\")\n",
    "print(f\"  Input folder: {'FOUND' if files_exist['input_folder'] else 'MISSING'} {INPUT_FOLDER}\")\n",
    "print(f\"  Ground truth: {'FOUND' if files_exist['ground_truth'] else 'MISSING'} {GROUND_TRUTH_FILE}\")\n",
    "\n",
    "if not files_exist['model']:\n",
    "    print(\"\\nModel checkpoint not found. Please:\")\n",
    "    print(\"   1. Download a model from Hugging Face or research repository\")\n",
    "    print(\"   2. Update MODEL_CHECKPOINT variable with correct path\")\n",
    "    print(\"   3. Supported formats: .safetensors, .pth, .pt\")\n",
    "\n",
    "if not files_exist['input_folder']:\n",
    "    print(\"\\nInput folder not found. Please:\")\n",
    "    print(\"   1. Create a folder with satellite images\")\n",
    "    print(\"   2. Update INPUT_FOLDER variable with correct path\")\n",
    "    print(\"   3. Supported formats: .jpg, .jpeg, .png, .tif, .tiff\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ready to run evaluation pipeline!\")\n",
    "print(\"Uncomment and run the cells below when files are ready.\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9fcdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model...\")\n",
    "model = load_satellite_model(\n",
    "    checkpoint_path=MODEL_CHECKPOINT,\n",
    "    architecture='resnet50',  # or 'resnet101', 'wide_resnet50_2'\n",
    "    num_classes=19,\n",
    "    input_channels=10\n",
    ")\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cb8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running batch inference...\")\n",
    "predictions = run_batch_inference(\n",
    "    model=model,\n",
    "    input_folder=INPUT_FOLDER,\n",
    "    output_file=os.path.join(OUTPUT_DIR, \"predictions.csv\"),\n",
    "    max_images=100  # Remove or set to None to process all images\n",
    ")\n",
    "print(\"Batch inference completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(GROUND_TRUTH_FILE):\n",
    "    print(\"Loading ground truth and computing metrics...\")\n",
    "    \n",
    "    # Load ground truth\n",
    "    ground_truth = evaluator.load_ground_truth(GROUND_TRUTH_FILE)\n",
    "    print(f\"Loaded ground truth for {len(ground_truth)} images\")\n",
    "    \n",
    "    # Compute comprehensive metrics\n",
    "    metrics = evaluator.compute_metrics(predictions, ground_truth)\n",
    "    print(\"Metrics computed!\")\n",
    "    \n",
    "    # Print evaluation summary\n",
    "    evaluator.print_evaluation_summary(metrics)\n",
    "    \n",
    "    # Generate comprehensive report with visualizations\n",
    "    generate_evaluation_report(metrics, OUTPUT_DIR)\n",
    "    \n",
    "else:\n",
    "    print(\"No ground truth file found. Skipping accuracy evaluation.\")\n",
    "    print(\"Predictions saved to CSV for manual review.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4687e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SATELLITE LAND COVER CLASSIFICATION EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'metrics' in locals():\n",
    "    print(\"\\nEVALUATION RESULTS SUMMARY:\")\n",
    "    print(f\"  - Overall Accuracy: {metrics['overall_metrics']['accuracy']:.4f}\")\n",
    "    print(f\"  - Macro F1-Score: {metrics['overall_metrics']['macro_f1']:.4f}\")\n",
    "    print(f\"  - Images Evaluated: {metrics['overall_metrics']['num_samples']}\")\n",
    "    print(f\"\\nResults saved in: {OUTPUT_DIR}/\")\n",
    "    print(\"  - Confusion matrix visualization\")\n",
    "    print(\"  - Per-class performance charts\") \n",
    "    print(\"  - Confidence analysis plots\")\n",
    "    print(\"  - Detailed metrics (JSON)\")\n",
    "    print(\"  - Evaluation summary report\")\n",
    "\n",
    "print(f\"\\nPredictions saved to: {OUTPUT_DIR}/predictions.csv\")\n",
    "print(\"\\nThis completes the downstream evaluation for satellite land cover classification.\")\n",
    "print(\"Use the generated metrics and visualizations to assess restoration model quality.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
