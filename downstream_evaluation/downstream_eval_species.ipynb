{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea8e1ef7",
   "metadata": {},
   "source": [
    "# Downstream Evaluation: Species Classification with SpeciesNet\n",
    "\n",
    "Wildlife species classification from camera trap images represents a crucial downstream task for evaluating image restoration models in ecological applications. This task serves as an excellent testbed for restoration quality assessment because:\n",
    "\n",
    "1. **Ecological Impact**: Accurate species classification directly supports biodiversity monitoring, conservation efforts, and wildlife research\n",
    "2. **Challenging Conditions**: Camera trap images often suffer from motion blur, low light, weather artifacts, and partial occlusions, making restoration particularly valuable\n",
    "3. **Fine-grained Recognition**: Distinguishing between closely related species requires preservation of subtle visual features that could be lost during restoration\n",
    "4. **Real-world Variability**: Images span diverse environments, lighting conditions, and animal poses, providing comprehensive restoration evaluation\n",
    "\n",
    "This evaluation framework uses Google's SpeciesNet ensemble, combining MegaDetector for animal detection with a specialized species classifier, to assess how well restored camera trap images maintain the critical visual information needed for accurate wildlife identification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca224cb4",
   "metadata": {},
   "source": [
    "## Library Imports and Environment Setup\n",
    "\n",
    "This section imports all necessary libraries for the SpeciesNet evaluation pipeline. The imports encompass data processing (pandas, numpy), visualization (matplotlib, seaborn), machine learning evaluation (scikit-learn), and specialized tools for handling taxonomic data and camera trap imagery.\n",
    "\n",
    "**Key Library Categories:**\n",
    "- **Data Processing**: pandas for structured data handling, numpy for numerical computations\n",
    "- **Visualization**: matplotlib and seaborn for comprehensive plotting and analysis visualization\n",
    "- **Evaluation Metrics**: scikit-learn for standard classification metrics and confusion matrices\n",
    "- **Image Processing**: PIL for image loading and basic processing operations\n",
    "- **Utility Libraries**: pathlib for file system operations, collections for data structures\n",
    "\n",
    "The environment setup ensures reproducible results and provides clear feedback about the readiness of the evaluation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d68bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from collections import defaultdict, Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports for evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, \n",
    "    classification_report, confusion_matrix,\n",
    "    top_k_accuracy_score\n",
    ")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"Ready for species classification evaluation with SpeciesNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc01f43",
   "metadata": {},
   "source": [
    "## Configuration and Setup\n",
    "\n",
    "This section establishes the configuration parameters for the SpeciesNet evaluation pipeline. The configuration encompasses file paths, model settings, and evaluation parameters that control the entire downstream evaluation process.\n",
    "\n",
    "**Key Configuration Components:**\n",
    "- **File Paths**: Locations of camera trap images, ground truth annotations, and output files\n",
    "- **SpeciesNet Parameters**: Model version, geographic constraints, and processing options\n",
    "- **Evaluation Settings**: Metrics to compute, taxonomic levels to analyze, and visualization parameters\n",
    "\n",
    "The configuration is designed to be easily adaptable to different datasets and evaluation scenarios while maintaining consistency with the iWildCam dataset format and SpeciesNet requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f515dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Paths - Update these with your actual paths\n",
    "        self.images_folder = \"Wildlife_Classification/nighttime_low_confidence\"  # Folder containing camera trap images\n",
    "        self.ground_truth_csv = \"Wildlife_Classification/ground_truth.csv\"  # CSV file with ground truth labels\n",
    "        self.predictions_json = \"Wildlife_Classification/predictions.json\"  # Output file for SpeciesNet predictions\n",
    "        \n",
    "        # SpeciesNet Configuration\n",
    "        self.country_code = None  # Optional: 3-letter ISO country code (e.g., \"USA\", \"CAN\", \"BRA\")\n",
    "        self.admin1_region = None  # Optional: Statgrounde code for US (e.g., \"CA\", \"TX\")\n",
    "        self.model_version = \"kaggle:google/speciesnet/pyTorch/v4.0.1a\"  # SpeciesNet model version\n",
    "        \n",
    "        # Evaluation Parameters\n",
    "        self.top_k_accuracy = [1, 3, 5]  # Top-k accuracy metrics to compute\n",
    "        self.taxonomic_levels = ['species', 'genus', 'family', 'order', 'class', 'phylum', 'kingdom']\n",
    "        \n",
    "        # Visualization Parameters\n",
    "        self.max_samples_to_show = 10  # Number of sample images to display\n",
    "        self.figure_size = (12, 8)\n",
    "        self.confusion_matrix_size = (15, 12)\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "print(\"Configuration initialized!\")\n",
    "print(f\"Update the paths in the Config class above:\")\n",
    "print(f\"- images_folder: {config.images_folder}\")\n",
    "print(f\"- ground_truth_csv: {config.ground_truth_csv}\")\n",
    "print(f\"- predictions_json: {config.predictions_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2684eb90",
   "metadata": {},
   "source": [
    "## Installation and Dependencies\n",
    "\n",
    "### SpeciesNet Package Installation\n",
    "\n",
    "Install the SpeciesNet Python package for species classification:\n",
    "\n",
    "```bash\n",
    "pip install speciesnet\n",
    "```\n",
    "\n",
    "**Note for Mac users:** If you encounter installation errors, use:\n",
    "```bash\n",
    "pip install speciesnet --use-pep517\n",
    "```\n",
    "\n",
    "**Verify Installation:**\n",
    "```bash\n",
    "python -m speciesnet.scripts.run_model --help\n",
    "```\n",
    "\n",
    "### Required Files and Dependencies\n",
    "\n",
    "**Model Components:**\n",
    "- **MegaDetector v5**: Automatically downloaded with SpeciesNet package\n",
    "- **SpeciesNet Classifier**: EfficientNet V2 M model for species classification\n",
    "- **Taxonomy Database**: Built-in taxonomic hierarchy for 2000+ species\n",
    "\n",
    "**Dataset Requirements:**\n",
    "- Camera trap images in standard formats (JPEG, PNG)\n",
    "- Ground truth annotations with species labels\n",
    "- Optional: Geographic metadata (country code, region) for improved accuracy\n",
    "\n",
    "**Key Citations:**\n",
    "- Beery, S., et al. (2021). \"The iWildCam 2021 Competition Dataset.\" NeurIPS Datasets and Benchmarks Track.\n",
    "- Willi, M., et al. (2019). \"Identifying animal species in camera trap images using deep learning and citizen science.\" Methods in Ecology and Evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af0dbd3",
   "metadata": {},
   "source": [
    "## SpeciesNet Prediction Pipeline\n",
    "\n",
    "This section implements the core SpeciesNet prediction pipeline for processing camera trap images. SpeciesNet is a two-stage ensemble system that combines animal detection with species classification to provide robust wildlife identification.\n",
    "\n",
    "**Pipeline Architecture:**\n",
    "- **Stage 1 - MegaDetector**: Detects and localizes animals, people, and vehicles in camera trap images\n",
    "- **Stage 2 - Species Classifier**: Classifies detected animals into 2000+ species using an EfficientNet V2 model\n",
    "- **Geographic Integration**: Optional geographic priors to improve classification accuracy based on species distribution data\n",
    "- **Ensemble Logic**: Combines detection confidence, classification scores, and geographic likelihood for final predictions\n",
    "\n",
    "**Key Features:**\n",
    "- **Automated Model Download**: Automatically retrieves pre-trained model weights on first use\n",
    "- **Batch Processing**: Efficient processing of large image datasets with progress tracking\n",
    "- **Error Handling**: Robust handling of corrupted images, failed detections, and processing errors\n",
    "- **Flexible Output**: Structured JSON output with detailed prediction information and confidence scores\n",
    "\n",
    "The pipeline is designed to handle the challenging conditions typical of camera trap imagery, including poor lighting, motion blur, partial occlusions, and diverse environmental contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cc37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesNetPredictor:\n",
    "    \"\"\"\n",
    "    Wrapper class for running SpeciesNet predictions on camera trap images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.predictions = None\n",
    "        \n",
    "    def check_speciesnet_installation(self):\n",
    "        \"\"\"Check if SpeciesNet is installed and can be imported.\"\"\"\n",
    "        try:\n",
    "            import speciesnet\n",
    "            print(\"SpeciesNet is installed and available\")\n",
    "            return True\n",
    "        except ImportError:\n",
    "            print(\"SpeciesNet is not installed.\")\n",
    "            print(\"Please install it using: pip install speciesnet\")\n",
    "            return False\n",
    "    \n",
    "    def prepare_image_list(self):\n",
    "        \"\"\"\n",
    "        Prepare a list of images in the format expected by SpeciesNet.\n",
    "        Returns a list of image file paths.\n",
    "        \"\"\"\n",
    "        images_path = Path(self.config.images_folder)\n",
    "        if not images_path.exists():\n",
    "            raise FileNotFoundError(f\"Images folder not found: {self.config.images_folder}\")\n",
    "        \n",
    "        # Find all image files\n",
    "        image_extensions = {'.jpg', '.jpeg', '.png', '.tiff', '.tif'}\n",
    "        image_files = []\n",
    "        \n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(images_path.glob(f\"*{ext}\"))\n",
    "            image_files.extend(images_path.glob(f\"*{ext.upper()}\"))\n",
    "        \n",
    "        print(f\"Found {len(image_files)} images in {self.config.images_folder}\")\n",
    "        return [str(img) for img in image_files]\n",
    "    \n",
    "    def run_speciesnet_command(self):\n",
    "        \"\"\"\n",
    "        Generate and display the SpeciesNet command that should be run.\n",
    "        \"\"\"\n",
    "        cmd_parts = [\n",
    "            \"python -m speciesnet.scripts.run_model\",\n",
    "            f'--folders \"{self.config.images_folder}\"',\n",
    "            f'--predictions_json \"{self.config.predictions_json}\"'\n",
    "        ]\n",
    "        \n",
    "        if self.config.country_code:\n",
    "            cmd_parts.append(f\"--country {self.config.country_code}\")\n",
    "        \n",
    "        if self.config.admin1_region:\n",
    "            cmd_parts.append(f\"--admin1_region {self.config.admin1_region}\")\n",
    "        \n",
    "        if self.config.model_version != \"kaggle:google/speciesnet/pyTorch/v4.0.1a\":\n",
    "            cmd_parts.append(f\"--model {self.config.model_version}\")\n",
    "        \n",
    "        command = \" \".join(cmd_parts)\n",
    "        \n",
    "        print(\"Run the following command in your terminal to generate SpeciesNet predictions:\")\n",
    "        print(\"\" + \"=\"*80)\n",
    "        print(command)\n",
    "        print(\"=\"*80 + \"\")\n",
    "        \n",
    "        return command\n",
    "    \n",
    "    def load_predictions(self):\n",
    "        \"\"\"\n",
    "        Load SpeciesNet predictions from the JSON file.\n",
    "        \"\"\"\n",
    "        predictions_path = Path(self.config.predictions_json)\n",
    "        if not predictions_path.exists():\n",
    "            raise FileNotFoundError(f\"Predictions file not found: {self.config.predictions_json}\")\n",
    "        \n",
    "        with open(predictions_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.predictions = data.get('predictions', [])\n",
    "        print(f\"Loaded {len(self.predictions)} predictions from {self.config.predictions_json}\")\n",
    "        \n",
    "        return self.predictions\n",
    "    \n",
    "    def get_prediction_summary(self):\n",
    "        \"\"\"\n",
    "        Get a summary of the predictions.\n",
    "        \"\"\"\n",
    "        if not self.predictions:\n",
    "            print(\"No predictions loaded. Run load_predictions() first.\")\n",
    "            return\n",
    "        \n",
    "        # Count successful vs failed predictions\n",
    "        successful = [p for p in self.predictions if 'failures' not in p]\n",
    "        failed = [p for p in self.predictions if 'failures' in p]\n",
    "        \n",
    "        print(f\"Prediction Summary:\")\n",
    "        print(f\"- Total images: {len(self.predictions)}\")\n",
    "        print(f\"- Successful predictions: {len(successful)}\")\n",
    "        print(f\"- Failed predictions: {len(failed)}\")\n",
    "        \n",
    "        if failed:\n",
    "            failure_types = Counter()\n",
    "            for p in failed:\n",
    "                for failure in p.get('failures', []):\n",
    "                    failure_types[failure] += 1\n",
    "            \n",
    "            print(\"Failure types:\")\n",
    "            for failure_type, count in failure_types.items():\n",
    "                print(f\"- {failure_type}: {count}\")\n",
    "        \n",
    "        # Count prediction types\n",
    "        if successful:\n",
    "            prediction_counts = Counter()\n",
    "            for p in successful:\n",
    "                pred = p.get('prediction', 'unknown')\n",
    "                prediction_counts[pred] += 1\n",
    "            \n",
    "            print(f\"Top 10 most common predictions:\")\n",
    "            for pred, count in prediction_counts.most_common(10):\n",
    "                print(f\"- {pred}: {count}\")\n",
    "\n",
    "# Initialize the predictor\n",
    "predictor = SpeciesNetPredictor(config)\n",
    "\n",
    "# Check if SpeciesNet is installed\n",
    "if predictor.check_speciesnet_installation():\n",
    "    print(\"You can proceed with generating predictions.\")\n",
    "else:\n",
    "    print(\"Please install SpeciesNet first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2f9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SpeciesNet Command\n",
    "print(\"Step 1: Generate SpeciesNet predictions\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Display the command to run\n",
    "command = predictor.run_speciesnet_command()\n",
    "\n",
    "print(\"After running the command in terminal, come back to this notebook and continue with the evaluation.\")\n",
    "print(\"Note: SpeciesNet will automatically download model weights on first run.\")\n",
    "print(\"The prediction process may take several minutes depending on the number of images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec702368",
   "metadata": {},
   "source": [
    "## Ground Truth Processing and Data Alignment\n",
    "\n",
    "This section handles the complex task of processing ground truth annotations and aligning them with SpeciesNet predictions. The evaluation framework supports the iWildCam dataset format and provides robust taxonomic parsing capabilities.\n",
    "\n",
    "**Ground Truth Processing:**\n",
    "- **Taxonomic Parsing**: Extracts hierarchical taxonomic information from structured labels (Kingdom > Phylum > Class > Order > Family > Genus > Species)\n",
    "- **Label Standardization**: Handles various label formats and missing taxonomic levels\n",
    "- **Data Validation**: Ensures consistency and completeness of taxonomic annotations\n",
    "\n",
    "**Data Alignment Process:**\n",
    "- **Filename Matching**: Aligns predictions with ground truth based on image filenames\n",
    "- **Missing Data Handling**: Identifies and reports images with missing predictions or ground truth\n",
    "- **Error Tracking**: Maintains detailed records of failed predictions and their causes\n",
    "\n",
    "**Taxonomic Hierarchy Support:**\n",
    "The framework maintains full taxonomic hierarchy for comprehensive evaluation at multiple levels of biological classification. This enables evaluation of both fine-grained species identification and broader taxonomic group classification, which is crucial for understanding model performance across different levels of biological similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5fa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxonomicProcessor:\n",
    "    \"\"\"\n",
    "    Process taxonomic labels and handle different taxonomic levels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.taxonomic_levels = ['kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "        self.common_name_level = 'common_name'\n",
    "    \n",
    "    def parse_taxonomic_label(self, label: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Parse taxonomic label in format: kingdom;phylum;class;order;family;genus;species;common_name\n",
    "        Returns a dictionary with taxonomic levels as keys.\n",
    "        \"\"\"\n",
    "        parts = label.split(';')\n",
    "        taxonomy = {}\n",
    "        \n",
    "        # Map parts to taxonomic levels\n",
    "        for i, level in enumerate(self.taxonomic_levels):\n",
    "            if i < len(parts):\n",
    "                taxonomy[level] = parts[i].strip() if parts[i].strip() else 'unknown'\n",
    "            else:\n",
    "                taxonomy[level] = 'unknown'\n",
    "        \n",
    "        # Add common name if available\n",
    "        if len(parts) > len(self.taxonomic_levels):\n",
    "            taxonomy[self.common_name_level] = parts[len(self.taxonomic_levels)].strip()\n",
    "        else:\n",
    "            taxonomy[self.common_name_level] = 'unknown'\n",
    "        \n",
    "        return taxonomy\n",
    "    \n",
    "    def get_taxonomic_level(self, taxonomy: Dict[str, str], level: str) -> str:\n",
    "        \"\"\"\n",
    "        Get the taxonomic label at a specific level.\n",
    "        \"\"\"\n",
    "        return taxonomy.get(level, 'unknown')\n",
    "    \n",
    "    def rollup_to_level(self, taxonomy: Dict[str, str], target_level: str) -> str:\n",
    "        \"\"\"\n",
    "        Roll up taxonomy to a higher level (e.g., from species to genus).\n",
    "        \"\"\"\n",
    "        if target_level not in self.taxonomic_levels:\n",
    "            return 'unknown'\n",
    "        \n",
    "        return taxonomy.get(target_level, 'unknown')\n",
    "\n",
    "class SpeciesEvaluator:\n",
    "    \"\"\"\n",
    "    Main evaluation class for species classification using SpeciesNet.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.taxonomic_processor = TaxonomicProcessor()\n",
    "        self.ground_truth_df = None\n",
    "        self.predictions = None\n",
    "        self.evaluation_data = None\n",
    "    \n",
    "    def load_ground_truth(self):\n",
    "        \"\"\"\n",
    "        Load ground truth from CSV file in iWildCam 2022 format.\n",
    "        Expected columns: 'Nighttime Image', 'Ground Truth Label', 'Sequence ID'\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.config.ground_truth_csv):\n",
    "            raise FileNotFoundError(f\"Ground truth file not found: {self.config.ground_truth_csv}\")\n",
    "        \n",
    "        # Load CSV\n",
    "        df = pd.read_csv(self.config.ground_truth_csv)\n",
    "        \n",
    "        # Check for required columns\n",
    "        required_columns = ['Nighttime Image', 'Ground Truth Label']\n",
    "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "        \n",
    "        if missing_columns:\n",
    "            print(f\"Warning: Missing columns {missing_columns}\")\n",
    "            print(f\"Available columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Try to infer column mapping\n",
    "            image_col = None\n",
    "            label_col = None\n",
    "            \n",
    "            for col in df.columns:\n",
    "                if 'image' in col.lower() or 'filename' in col.lower():\n",
    "                    image_col = col\n",
    "                elif 'label' in col.lower() or 'ground' in col.lower():\n",
    "                    label_col = col\n",
    "            \n",
    "            if image_col and label_col:\n",
    "                print(f\"Using '{image_col}' as image column and '{label_col}' as label column\")\n",
    "                df = df.rename(columns={image_col: 'Nighttime Image', label_col: 'Ground Truth Label'})\n",
    "            else:\n",
    "                raise ValueError(\"Could not identify image and label columns\")\n",
    "        \n",
    "        # Parse taxonomic labels\n",
    "        print(\"Parsing taxonomic labels...\")\n",
    "        taxonomies = []\n",
    "        for _, row in df.iterrows():\n",
    "            taxonomy = self.taxonomic_processor.parse_taxonomic_label(row['Ground Truth Label'])\n",
    "            taxonomies.append(taxonomy)\n",
    "        \n",
    "        # Add taxonomic columns to dataframe\n",
    "        taxonomy_df = pd.DataFrame(taxonomies)\n",
    "        self.ground_truth_df = pd.concat([df, taxonomy_df], axis=1)\n",
    "        \n",
    "        print(f\"Loaded {len(self.ground_truth_df)} ground truth labels\")\n",
    "        print(f\"Parsed taxonomic information for {len(taxonomies)} labels\")\n",
    "        \n",
    "        # Show sample of taxonomic parsing\n",
    "        print(\"Sample taxonomic parsing:\")\n",
    "        for i in range(min(3, len(self.ground_truth_df))):\n",
    "            row = self.ground_truth_df.iloc[i]\n",
    "            print(f\"Image: {row['Nighttime Image']}\")\n",
    "            print(f\"Original: {row['Ground Truth Label']}\")\n",
    "            print(f\"Species: {row['species']}, Genus: {row['genus']}, Family: {row['family']}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        return self.ground_truth_df\n",
    "    \n",
    "    def load_predictions(self):\n",
    "        \"\"\"\n",
    "        Load SpeciesNet predictions from JSON file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.config.predictions_json):\n",
    "            raise FileNotFoundError(f\"Predictions file not found: {self.config.predictions_json}\")\n",
    "        \n",
    "        with open(self.config.predictions_json, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.predictions = data.get('predictions', [])\n",
    "        print(f\"Loaded {len(self.predictions)} SpeciesNet predictions\")\n",
    "        \n",
    "        return self.predictions\n",
    "    \n",
    "    def align_data(self):\n",
    "        \"\"\"\n",
    "        Align ground truth and predictions by filename.\n",
    "        \"\"\"\n",
    "        if self.ground_truth_df is None:\n",
    "            raise ValueError(\"Ground truth not loaded. Call load_ground_truth() first.\")\n",
    "        \n",
    "        if self.predictions is None:\n",
    "            raise ValueError(\"Predictions not loaded. Call load_predictions() first.\")\n",
    "        \n",
    "        # Create prediction lookup by filename\n",
    "        pred_lookup = {}\n",
    "        for pred in self.predictions:\n",
    "            filepath = pred.get('filepath', '')\n",
    "            filename = os.path.basename(filepath)\n",
    "            pred_lookup[filename] = pred\n",
    "        \n",
    "        # Align with ground truth\n",
    "        aligned_data = []\n",
    "        missing_predictions = []\n",
    "        \n",
    "        for _, row in self.ground_truth_df.iterrows():\n",
    "            filename = row['Nighttime Image']\n",
    "            \n",
    "            if filename in pred_lookup:\n",
    "                pred = pred_lookup[filename]\n",
    "                \n",
    "                # Extract prediction info\n",
    "                prediction = pred.get('prediction', 'unknown')\n",
    "                prediction_score = pred.get('prediction_score', 0.0)\n",
    "                failures = pred.get('failures', [])\n",
    "                \n",
    "                aligned_data.append({\n",
    "                    'filename': filename,\n",
    "                    'ground_truth_full': row['Ground Truth Label'],\n",
    "                    'gt_species': row['species'],\n",
    "                    'gt_genus': row['genus'],\n",
    "                    'gt_family': row['family'],\n",
    "                    'gt_order': row['order'],\n",
    "                    'gt_class': row['class'],\n",
    "                    'gt_phylum': row['phylum'],\n",
    "                    'gt_kingdom': row['kingdom'],\n",
    "                    'gt_common_name': row.get('common_name', 'unknown'),\n",
    "                    'prediction': prediction,\n",
    "                    'prediction_score': prediction_score,\n",
    "                    'failed': len(failures) > 0,\n",
    "                    'failure_types': failures\n",
    "                })\n",
    "            else:\n",
    "                missing_predictions.append(filename)\n",
    "        \n",
    "        self.evaluation_data = pd.DataFrame(aligned_data)\n",
    "        \n",
    "        print(f\"Aligned {len(self.evaluation_data)} image predictions with ground truth\")\n",
    "        \n",
    "        if missing_predictions:\n",
    "            print(f\"Missing predictions for {len(missing_predictions)} images\")\n",
    "            if len(missing_predictions) <= 10:\n",
    "                print(\"Missing predictions for:\", missing_predictions)\n",
    "            else:\n",
    "                print(f\"First 10 missing:\", missing_predictions[:10])\n",
    "        \n",
    "        return self.evaluation_data\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = SpeciesEvaluator(config)\n",
    "print(\"Species evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3f3ee",
   "metadata": {},
   "source": [
    "## Data Loading and Validation\n",
    "\n",
    "This section executes the data loading pipeline and performs comprehensive validation of the ground truth and prediction data. The process ensures data integrity and alignment before proceeding with evaluation metrics computation.\n",
    "\n",
    "**Data Loading Steps:**\n",
    "1. **Ground Truth Loading**: Reads and parses taxonomic annotations from CSV format\n",
    "2. **Prediction Loading**: Imports SpeciesNet results from JSON output files\n",
    "3. **Data Alignment**: Matches predictions with ground truth by filename\n",
    "4. **Validation Checks**: Identifies missing data, failed predictions, and format inconsistencies\n",
    "\n",
    "**Quality Assurance:**\n",
    "The framework performs extensive validation to ensure reliable evaluation results, including verification of taxonomic label parsing, prediction format consistency, and data completeness assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "print(\"Loading ground truth data...\")\n",
    "try:\n",
    "    ground_truth_df = evaluator.load_ground_truth()\n",
    "    print(\"Ground truth loaded successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please update the ground_truth_csv path in the configuration section above.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ground truth: {e}\")\n",
    "\n",
    "# Load SpeciesNet predictions\n",
    "print(\"Loading SpeciesNet predictions...\")\n",
    "try:\n",
    "    predictions = evaluator.load_predictions()\n",
    "    print(\"Predictions loaded successfully\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please run the SpeciesNet command first, then update the predictions_json path.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading predictions: {e}\")\n",
    "\n",
    "# Align ground truth and predictions\n",
    "print(\"Aligning ground truth with predictions...\")\n",
    "try:\n",
    "    evaluation_data = evaluator.align_data()\n",
    "    print(\"Data alignment completed\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    print(f\"Dataset Statistics:\")\n",
    "    print(f\"- Total aligned samples: {len(evaluation_data)}\")\n",
    "    print(f\"- Successful predictions: {len(evaluation_data[~evaluation_data['failed']])}\")\n",
    "    print(f\"- Failed predictions: {len(evaluation_data[evaluation_data['failed']])}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error aligning data: {e}\")\n",
    "\n",
    "# Display sample of aligned data\n",
    "if 'evaluation_data' in locals():\n",
    "    print(\"Sample of aligned data:\")\n",
    "    print(evaluation_data[['filename', 'gt_species', 'prediction', 'prediction_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6157cc6e",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Metrics\n",
    "\n",
    "This section implements a sophisticated evaluation framework specifically designed for wildlife species classification. The metrics account for the hierarchical nature of taxonomic classification and the unique challenges of camera trap image analysis.\n",
    "\n",
    "### Primary Evaluation Metrics\n",
    "\n",
    "**Species-Level Classification Metrics:**\n",
    "- **Accuracy**: Overall fraction of correctly identified species\n",
    "- **Precision (Weighted)**: Average precision across species, weighted by species frequency in the dataset\n",
    "- **Recall (Weighted)**: Average recall across species, weighted by species frequency\n",
    "- **F1-Score (Weighted)**: Harmonic mean of precision and recall, accounting for class imbalance\n",
    "\n",
    "**Hierarchical Taxonomic Accuracy:**\n",
    "- **Multi-level Accuracy**: Evaluates correctness at different taxonomic levels (Kingdom, Phylum, Class, Order, Family, Genus, Species)\n",
    "- **Taxonomic Distance**: Measures how \"close\" incorrect predictions are to the true species in taxonomic hierarchy\n",
    "- **Hierarchical Precision**: Considers predictions correct if they match at any taxonomic level above species\n",
    "\n",
    "**Confidence and Reliability Metrics:**\n",
    "- **Prediction Confidence Analysis**: Distribution of model confidence scores and their calibration\n",
    "- **Confidence-Accuracy Correlation**: Relationship between prediction confidence and actual accuracy\n",
    "- **Failure Mode Analysis**: Categorization and frequency of different types of prediction failures\n",
    "\n",
    "### Advanced Evaluation Features\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "Wildlife datasets typically exhibit severe class imbalance, with some species having thousands of examples while others have only a few. The evaluation framework uses weighted metrics to account for this imbalance and provides per-class performance analysis.\n",
    "\n",
    "**Taxonomic Hierarchy Awareness:**\n",
    "Unlike generic image classification, species identification benefits from taxonomic knowledge. A prediction of \"Gray Wolf\" when the true species is \"Red Wolf\" is more acceptable than predicting \"House Cat\" - both are taxonomically closer and ecologically more reasonable errors.\n",
    "\n",
    "**Geographic and Temporal Context:**\n",
    "The framework can incorporate geographic priors and temporal information when available, providing more nuanced evaluation that reflects real-world deployment scenarios where species distributions and seasonal patterns matter.\n",
    "\n",
    "**Error Pattern Analysis:**\n",
    "Systematic analysis of common misclassification patterns helps identify whether errors are due to visual similarity, taxonomic confusion, or systematic biases in the model or dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab09c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_taxonomic_metrics(evaluation_data: pd.DataFrame, taxonomic_level: str = 'species'):\n",
    "    \"\"\"\n",
    "    Compute classification metrics at a specific taxonomic level.\n",
    "    \"\"\"\n",
    "    # Filter out failed predictions\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']].copy()\n",
    "    \n",
    "    if len(successful_data) == 0:\n",
    "        print(f\"No successful predictions found for {taxonomic_level} level evaluation\")\n",
    "        return None\n",
    "    \n",
    "    # Get ground truth and predictions at the specified level\n",
    "    gt_column = f'gt_{taxonomic_level}'\n",
    "    \n",
    "    if gt_column not in successful_data.columns:\n",
    "        print(f\"Ground truth column {gt_column} not found\")\n",
    "        return None\n",
    "    \n",
    "    y_true = successful_data[gt_column].values\n",
    "    y_pred = successful_data['prediction'].values\n",
    "    \n",
    "    # Handle predictions that might not match the taxonomic level\n",
    "    # For SpeciesNet, predictions can be at any taxonomic level\n",
    "    processed_predictions = []\n",
    "    for pred in y_pred:\n",
    "        # SpeciesNet predictions might be common names or scientific names\n",
    "        # We'll use them as-is for now, but this could be enhanced with\n",
    "        # taxonomic matching/mapping\n",
    "        processed_predictions.append(pred)\n",
    "    \n",
    "    y_pred = processed_predictions\n",
    "    \n",
    "    # Get unique classes\n",
    "    all_classes = sorted(list(set(y_true) | set(y_pred)))\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Compute per-class metrics\n",
    "    per_class_metrics = classification_report(\n",
    "        y_true, y_pred, \n",
    "        labels=all_classes,\n",
    "        target_names=all_classes,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=all_classes)\n",
    "    \n",
    "    results = {\n",
    "        'taxonomic_level': taxonomic_level,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'num_samples': len(successful_data),\n",
    "        'num_classes': len(all_classes),\n",
    "        'per_class_metrics': per_class_metrics,\n",
    "        'confusion_matrix': cm,\n",
    "        'class_labels': all_classes,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_hierarchical_accuracy(evaluation_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Compute hierarchical accuracy where a prediction is considered correct\n",
    "    if it matches at any taxonomic level (e.g., correct genus even if species is wrong).\n",
    "    \"\"\"\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']].copy()\n",
    "    \n",
    "    if len(successful_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    taxonomic_levels = ['species', 'genus', 'family', 'order', 'class', 'phylum', 'kingdom']\n",
    "    hierarchical_matches = []\n",
    "    \n",
    "    for _, row in successful_data.iterrows():\n",
    "        prediction = row['prediction'].lower().strip()\n",
    "        matches_at_level = {}\n",
    "        \n",
    "        # Check if prediction matches at any taxonomic level\n",
    "        for level in taxonomic_levels:\n",
    "            gt_value = str(row[f'gt_{level}']).lower().strip()\n",
    "            \n",
    "            # Direct match\n",
    "            if prediction == gt_value:\n",
    "                matches_at_level[level] = True\n",
    "            # Partial match (prediction contains ground truth or vice versa)\n",
    "            elif prediction in gt_value or gt_value in prediction:\n",
    "                matches_at_level[level] = True\n",
    "            else:\n",
    "                matches_at_level[level] = False\n",
    "        \n",
    "        hierarchical_matches.append(matches_at_level)\n",
    "    \n",
    "    # Calculate accuracy at each level\n",
    "    hierarchical_accuracy = {}\n",
    "    for level in taxonomic_levels:\n",
    "        correct_at_level = sum([match[level] for match in hierarchical_matches])\n",
    "        hierarchical_accuracy[level] = correct_at_level / len(hierarchical_matches)\n",
    "    \n",
    "    return hierarchical_accuracy\n",
    "\n",
    "# Compute metrics if data is available\n",
    "if 'evaluation_data' in locals() and len(evaluation_data) > 0:\n",
    "    print(\"Computing evaluation metrics...\")\n",
    "    \n",
    "    # Compute species-level metrics (primary evaluation)\n",
    "    species_metrics = compute_taxonomic_metrics(evaluation_data, 'species')\n",
    "    \n",
    "    if species_metrics:\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"SPECIES-LEVEL CLASSIFICATION METRICS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Accuracy: {species_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Precision (weighted): {species_metrics['precision']:.4f}\")\n",
    "        print(f\"Recall (weighted): {species_metrics['recall']:.4f}\")\n",
    "        print(f\"F1-Score (weighted): {species_metrics['f1_score']:.4f}\")\n",
    "        print(f\"Number of samples: {species_metrics['num_samples']}\")\n",
    "        print(f\"Number of classes: {species_metrics['num_classes']}\")\n",
    "    \n",
    "    # Compute hierarchical accuracy\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"HIERARCHICAL ACCURACY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    hierarchical_acc = compute_hierarchical_accuracy(evaluation_data)\n",
    "    if hierarchical_acc:\n",
    "        for level, acc in hierarchical_acc.items():\n",
    "            print(f\"{level.capitalize()}: {acc:.4f}\")\n",
    "    \n",
    "    # Compute metrics at other taxonomic levels\n",
    "    other_levels = ['genus', 'family', 'order']\n",
    "    other_metrics = {}\n",
    "    \n",
    "    for level in other_levels:\n",
    "        metrics = compute_taxonomic_metrics(evaluation_data, level)\n",
    "        if metrics:\n",
    "            other_metrics[level] = metrics\n",
    "            print(f\"{level.upper()}-LEVEL ACCURACY: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Evaluation data not available. Please load and align data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b5426e",
   "metadata": {},
   "source": [
    "## Visualization and Performance Analysis\n",
    "\n",
    "This section provides comprehensive visualization tools for understanding SpeciesNet performance patterns and identifying areas for improvement. The visualizations are specifically designed for wildlife classification analysis and taxonomic evaluation.\n",
    "\n",
    "**Visualization Components:**\n",
    "\n",
    "**Distribution Analysis:**\n",
    "- **Species Distribution Plots**: Compare the frequency distribution of predicted vs ground truth species\n",
    "- **Taxonomic Level Comparisons**: Side-by-side analysis of predictions and ground truth at different taxonomic levels\n",
    "- **Geographic Distribution**: Spatial analysis of prediction accuracy across different regions (when geographic data is available)\n",
    "\n",
    "**Performance Visualizations:**\n",
    "- **Hierarchical Accuracy Charts**: Bar charts showing accuracy at each taxonomic level\n",
    "- **Confusion Matrices**: Heat maps showing classification patterns, with options for different taxonomic levels\n",
    "- **Confidence Distribution Plots**: Histograms and box plots of prediction confidence scores\n",
    "\n",
    "**Error Analysis Visualizations:**\n",
    "- **Error Pattern Heat Maps**: Visualization of common species confusion patterns\n",
    "- **Confidence vs Accuracy Scatter Plots**: Analysis of model calibration and overconfidence\n",
    "- **Failure Mode Distribution**: Categorical analysis of why predictions fail\n",
    "\n",
    "**Interactive Sample Analysis:**\n",
    "- **Sample Image Grids**: Display representative images with predictions and ground truth\n",
    "- **Error Case Studies**: Detailed examination of common misclassification patterns\n",
    "- **Confidence-based Sampling**: Show high/low confidence predictions for manual inspection\n",
    "\n",
    "These visualizations are essential for understanding model behavior, identifying systematic biases, and making informed decisions about model deployment and improvement strategies in wildlife monitoring applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b77627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_distribution(evaluation_data: pd.DataFrame, top_n: int = 20):\n",
    "    \"\"\"\n",
    "    Plot distribution of predictions and ground truth labels.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Successful predictions only\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']]\n",
    "    \n",
    "    # Ground truth distribution\n",
    "    gt_counts = successful_data['gt_species'].value_counts().head(top_n)\n",
    "    ax1.barh(range(len(gt_counts)), gt_counts.values)\n",
    "    ax1.set_yticks(range(len(gt_counts)))\n",
    "    ax1.set_yticklabels(gt_counts.index, fontsize=10)\n",
    "    ax1.set_xlabel('Count')\n",
    "    ax1.set_title(f'Top {top_n} Ground Truth Species')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # Prediction distribution\n",
    "    pred_counts = successful_data['prediction'].value_counts().head(top_n)\n",
    "    ax2.barh(range(len(pred_counts)), pred_counts.values, color='orange')\n",
    "    ax2.set_yticks(range(len(pred_counts)))\n",
    "    ax2.set_yticklabels(pred_counts.index, fontsize=10)\n",
    "    ax2.set_xlabel('Count')\n",
    "    ax2.set_title(f'Top {top_n} SpeciesNet Predictions')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_hierarchical_accuracy(hierarchical_acc: Dict[str, float]):\n",
    "    \"\"\"\n",
    "    Plot hierarchical accuracy across taxonomic levels.\n",
    "    \"\"\"\n",
    "    if not hierarchical_acc:\n",
    "        print(\"No hierarchical accuracy data available\")\n",
    "        return\n",
    "    \n",
    "    levels = list(hierarchical_acc.keys())\n",
    "    accuracies = list(hierarchical_acc.values())\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(levels, accuracies, color='skyblue', alpha=0.8, edgecolor='navy')\n",
    "    plt.xlabel('Taxonomic Level')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Hierarchical Accuracy Across Taxonomic Levels')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, acc in zip(bars, accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix_subset(cm, class_labels, top_n: int = 15):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for top N most common classes.\n",
    "    \"\"\"\n",
    "    if len(class_labels) <= top_n:\n",
    "        # Use all classes if we have fewer than top_n\n",
    "        selected_indices = list(range(len(class_labels)))\n",
    "        selected_labels = class_labels\n",
    "        selected_cm = cm\n",
    "    else:\n",
    "        # Select top N classes by support (sum of rows)\n",
    "        class_support = cm.sum(axis=1)\n",
    "        top_indices = np.argsort(class_support)[-top_n:][::-1]\n",
    "        \n",
    "        selected_indices = top_indices\n",
    "        selected_labels = [class_labels[i] for i in top_indices]\n",
    "        selected_cm = cm[np.ix_(top_indices, top_indices)]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Normalize confusion matrix\n",
    "    cm_normalized = selected_cm.astype('float') / selected_cm.sum(axis=1)[:, np.newaxis]\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)  # Handle division by zero\n",
    "    \n",
    "    sns.heatmap(cm_normalized, \n",
    "                xticklabels=selected_labels, \n",
    "                yticklabels=selected_labels,\n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap='Blues',\n",
    "                cbar_kws={'label': 'Normalized Count'})\n",
    "    \n",
    "    plt.xlabel('Predicted Species')\n",
    "    plt.ylabel('True Species')\n",
    "    plt.title(f'Confusion Matrix - Top {len(selected_labels)} Most Common Species')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confidence_distribution(evaluation_data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot distribution of prediction confidence scores.\n",
    "    \"\"\"\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']]\n",
    "    \n",
    "    if len(successful_data) == 0:\n",
    "        print(\"No successful predictions to analyze\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Overall confidence distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(successful_data['prediction_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Prediction Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of SpeciesNet Confidence Scores')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Confidence vs accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Create confidence bins\n",
    "    bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    bin_accuracies = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(len(bins)-1):\n",
    "        mask = (successful_data['prediction_score'] >= bins[i]) & (successful_data['prediction_score'] < bins[i+1])\n",
    "        if i == len(bins)-2:  # Include the last bin's right edge\n",
    "            mask = (successful_data['prediction_score'] >= bins[i]) & (successful_data['prediction_score'] <= bins[i+1])\n",
    "        \n",
    "        bin_data = successful_data[mask]\n",
    "        if len(bin_data) > 0:\n",
    "            # Simple accuracy check (this could be enhanced with proper taxonomic matching)\n",
    "            correct = (bin_data['prediction'].str.lower() == bin_data['gt_species'].str.lower()).sum()\n",
    "            accuracy = correct / len(bin_data)\n",
    "            bin_accuracies.append(accuracy)\n",
    "            bin_counts.append(len(bin_data))\n",
    "        else:\n",
    "            bin_accuracies.append(0)\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    # Plot confidence vs accuracy\n",
    "    plt.scatter(bin_centers, bin_accuracies, s=[c*3 for c in bin_counts], alpha=0.7, color='orange')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Confidence vs Accuracy')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations if data is available\n",
    "if 'evaluation_data' in locals() and len(evaluation_data) > 0:\n",
    "    print(\"Generating visualizations...\")\n",
    "    \n",
    "    # 1. Prediction distribution\n",
    "    plot_prediction_distribution(evaluation_data)\n",
    "    \n",
    "    # 2. Hierarchical accuracy\n",
    "    if 'hierarchical_acc' in locals():\n",
    "        plot_hierarchical_accuracy(hierarchical_acc)\n",
    "    \n",
    "    # 3. Confusion matrix\n",
    "    if 'species_metrics' in locals() and species_metrics:\n",
    "        plot_confusion_matrix_subset(species_metrics['confusion_matrix'], \n",
    "                                   species_metrics['class_labels'])\n",
    "    \n",
    "    # 4. Confidence distribution\n",
    "    plot_confidence_distribution(evaluation_data)\n",
    "    \n",
    "else:\n",
    "    print(\"No evaluation data available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8373dc62",
   "metadata": {},
   "source": [
    "## Sample Image Analysis and Error Investigation\n",
    "\n",
    "This section provides detailed visual analysis capabilities for understanding model performance through direct examination of camera trap images. Visual inspection is crucial for wildlife classification evaluation as it reveals patterns that purely numerical metrics might miss.\n",
    "\n",
    "**Sample Analysis Features:**\n",
    "\n",
    "**Stratified Sampling:**\n",
    "- **Correct Classifications**: Examples where SpeciesNet correctly identified the species\n",
    "- **Incorrect Classifications**: Cases where the prediction was wrong, with analysis of error types\n",
    "- **High Confidence Samples**: Images where the model was very confident (confidence > 0.8)\n",
    "- **Low Confidence Samples**: Uncertain predictions that might require human review\n",
    "- **Mixed Sampling**: Representative samples across all categories for comprehensive overview\n",
    "\n",
    "**Error Pattern Investigation:**\n",
    "- **Taxonomic Error Analysis**: Examination of whether errors occur within the same taxonomic family/genus\n",
    "- **Visual Similarity Confusions**: Cases where species are visually similar but taxonomically different\n",
    "- **Environmental Factor Analysis**: How lighting, weather, or habitat affects classification accuracy\n",
    "- **Pose and Occlusion Effects**: Impact of animal positioning and partial visibility on predictions\n",
    "\n",
    "**Quality Assessment:**\n",
    "Visual inspection helps validate the evaluation metrics by providing context for numerical results. It reveals whether model errors are \"reasonable\" (confusing similar-looking species) or indicate systematic issues (consistently misidentifying distinctive species)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371cb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_images(evaluation_data: pd.DataFrame, images_folder: str, \n",
    "                         sample_type: str = 'mixed', n_samples: int = 6):\n",
    "    \"\"\"\n",
    "    Display sample images with their predictions and ground truth.\n",
    "    \n",
    "    sample_type options:\n",
    "    - 'correct': Show correctly classified images\n",
    "    - 'incorrect': Show incorrectly classified images  \n",
    "    - 'high_confidence': Show high confidence predictions\n",
    "    - 'low_confidence': Show low confidence predictions\n",
    "    - 'mixed': Show a mix of different cases\n",
    "    \"\"\"\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']]\n",
    "    \n",
    "    if len(successful_data) == 0:\n",
    "        print(\"No successful predictions to display\")\n",
    "        return\n",
    "    \n",
    "    # Create simple accuracy check (could be enhanced with taxonomic matching)\n",
    "    successful_data = successful_data.copy()\n",
    "    successful_data['simple_correct'] = (\n",
    "        successful_data['prediction'].str.lower().str.strip() == \n",
    "        successful_data['gt_species'].str.lower().str.strip()\n",
    "    )\n",
    "    \n",
    "    # Select samples based on type\n",
    "    if sample_type == 'correct':\n",
    "        sample_data = successful_data[successful_data['simple_correct']]\n",
    "        title_suffix = \"Correctly Classified\"\n",
    "    elif sample_type == 'incorrect':\n",
    "        sample_data = successful_data[~successful_data['simple_correct']]\n",
    "        title_suffix = \"Incorrectly Classified\"\n",
    "    elif sample_type == 'high_confidence':\n",
    "        sample_data = successful_data[successful_data['prediction_score'] > 0.8]\n",
    "        title_suffix = \"High Confidence (>0.8)\"\n",
    "    elif sample_type == 'low_confidence':\n",
    "        sample_data = successful_data[successful_data['prediction_score'] < 0.3]\n",
    "        title_suffix = \"Low Confidence (<0.3)\"\n",
    "    else:  # mixed\n",
    "        sample_data = successful_data\n",
    "        title_suffix = \"Mixed Sample\"\n",
    "    \n",
    "    if len(sample_data) == 0:\n",
    "        print(f\"No samples found for type: {sample_type}\")\n",
    "        return\n",
    "    \n",
    "    # Sample randomly\n",
    "    sample_indices = np.random.choice(len(sample_data), \n",
    "                                    size=min(n_samples, len(sample_data)), \n",
    "                                    replace=False)\n",
    "    samples = sample_data.iloc[sample_indices]\n",
    "    \n",
    "    # Create subplot grid\n",
    "    n_cols = 3\n",
    "    n_rows = (len(samples) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(f'Sample Images - {title_suffix}', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for idx, (_, row) in enumerate(samples.iterrows()):\n",
    "        ax = axes[idx // n_cols, idx % n_cols]\n",
    "        \n",
    "        # Try to load and display image\n",
    "        image_path = os.path.join(images_folder, row['filename'])\n",
    "        \n",
    "        try:\n",
    "            if os.path.exists(image_path):\n",
    "                img = Image.open(image_path)\n",
    "                ax.imshow(img)\n",
    "            else:\n",
    "                # Create placeholder if image not found\n",
    "                ax.text(0.5, 0.5, f\"Image not found:{row['filename']}\", \n",
    "                       ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "        except Exception as e:\n",
    "            ax.text(0.5, 0.5, f\"Error loading image:{str(e)}\", \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "        \n",
    "        # Add title with prediction info\n",
    "        gt_species = row['gt_species']\n",
    "        prediction = row['prediction']\n",
    "        confidence = row['prediction_score']\n",
    "        \n",
    "        title = f\"GT: {gt_species}Pred: {prediction}Conf: {confidence:.3f}\"\n",
    "        \n",
    "        # Color code the title\n",
    "        if row['simple_correct']:\n",
    "            title_color = 'green'\n",
    "        else:\n",
    "            title_color = 'red'\n",
    "        \n",
    "        ax.set_title(title, fontsize=10, color=title_color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(samples), n_rows * n_cols):\n",
    "        axes[idx // n_cols, idx % n_cols].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def analyze_prediction_errors(evaluation_data: pd.DataFrame, top_n: int = 10):\n",
    "    \"\"\"\n",
    "    Analyze common prediction errors.\n",
    "    \"\"\"\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']].copy()\n",
    "    \n",
    "    if len(successful_data) == 0:\n",
    "        print(\"No successful predictions to analyze\")\n",
    "        return\n",
    "    \n",
    "    # Simple accuracy check\n",
    "    successful_data['simple_correct'] = (\n",
    "        successful_data['prediction'].str.lower().str.strip() == \n",
    "        successful_data['gt_species'].str.lower().str.strip()\n",
    "    )\n",
    "    \n",
    "    incorrect_data = successful_data[~successful_data['simple_correct']]\n",
    "    \n",
    "    if len(incorrect_data) == 0:\n",
    "        print(\"No incorrect predictions found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"PREDICTION ERROR ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Total samples: {len(successful_data)}\")\n",
    "    print(f\"Correct predictions: {len(successful_data[successful_data['simple_correct']])}\")\n",
    "    print(f\"Incorrect predictions: {len(incorrect_data)}\")\n",
    "    print(f\"Simple accuracy: {len(successful_data[successful_data['simple_correct']]) / len(successful_data):.4f}\")\n",
    "    \n",
    "    # Most common error patterns\n",
    "    print(f\"Most common error patterns:\")\n",
    "    error_patterns = incorrect_data.groupby(['gt_species', 'prediction']).size().sort_values(ascending=False)\n",
    "    \n",
    "    for i, ((gt, pred), count) in enumerate(error_patterns.head(top_n).items()):\n",
    "        print(f\"{i+1:2d}. {gt}  {pred} ({count} times)\")\n",
    "    \n",
    "    # Most commonly confused ground truth species\n",
    "    print(f\"Most commonly misclassified ground truth species:\")\n",
    "    confused_gt = incorrect_data['gt_species'].value_counts().head(top_n)\n",
    "    \n",
    "    for i, (species, count) in enumerate(confused_gt.items()):\n",
    "        total_gt_count = successful_data[successful_data['gt_species'] == species].shape[0]\n",
    "        error_rate = count / total_gt_count if total_gt_count > 0 else 0\n",
    "        print(f\"{i+1:2d}. {species}: {count}/{total_gt_count} errors (error rate: {error_rate:.3f})\")\n",
    "    \n",
    "    # Most common incorrect predictions\n",
    "    print(f\"Most common incorrect predictions:\")\n",
    "    wrong_preds = incorrect_data['prediction'].value_counts().head(top_n)\n",
    "    \n",
    "    for i, (prediction, count) in enumerate(wrong_preds.items()):\n",
    "        print(f\"{i+1:2d}. {prediction}: {count} times\")\n",
    "\n",
    "# Display sample images if data is available\n",
    "if 'evaluation_data' in locals() and len(evaluation_data) > 0:\n",
    "    print(\"Displaying sample images...\")\n",
    "    \n",
    "    # Show mixed samples first\n",
    "    display_sample_images(evaluation_data, config.images_folder, 'mixed', 6)\n",
    "    \n",
    "    # Analyze prediction errors\n",
    "    analyze_prediction_errors(evaluation_data)\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"Additional sample types you can explore:\")\n",
    "    print(\"- display_sample_images(evaluation_data, config.images_folder, 'correct', 6)\")\n",
    "    print(\"- display_sample_images(evaluation_data, config.images_folder, 'incorrect', 6)\")\n",
    "    print(\"- display_sample_images(evaluation_data, config.images_folder, 'high_confidence', 6)\")\n",
    "    print(\"- display_sample_images(evaluation_data, config.images_folder, 'low_confidence', 6)\")\n",
    "    \n",
    "else:\n",
    "    print(\"No evaluation data available for sample analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8f68e",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Summary and Export\n",
    "\n",
    "This section generates detailed evaluation reports and exports results in multiple formats for further analysis and integration with other research tools. The comprehensive summary provides actionable insights for model improvement and deployment decisions.\n",
    "\n",
    "**Summary Report Components:**\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Total number of images processed and success/failure rates\n",
    "- Species diversity and distribution statistics  \n",
    "- Quality assessment of ground truth annotations\n",
    "\n",
    "**Performance Summary:**\n",
    "- Overall accuracy metrics across all taxonomic levels\n",
    "- Confidence score statistics and calibration analysis\n",
    "- Identification of best and worst performing species categories\n",
    "\n",
    "**Error Analysis:**\n",
    "- Common misclassification patterns and their frequencies\n",
    "- Systematic error identification (consistent confusion between specific species pairs)\n",
    "- Failure mode categorization (detection failures vs classification errors)\n",
    "\n",
    "**Export Formats:**\n",
    "- **CSV Files**: Detailed per-image results for integration with other analysis tools\n",
    "- **JSON Summaries**: Structured metrics data for programmatic access\n",
    "- **Text Reports**: Human-readable comprehensive evaluation summaries\n",
    "\n",
    "**Deployment Recommendations:**\n",
    "Based on the evaluation results, the framework provides specific recommendations for:\n",
    "- Model deployment strategies in different scenarios\n",
    "- Confidence threshold optimization\n",
    "- Species-specific performance considerations\n",
    "- Areas requiring additional training data or model improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6acf98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_summary(evaluation_data: pd.DataFrame, \n",
    "                              species_metrics: Optional[Dict] = None,\n",
    "                              hierarchical_acc: Optional[Dict] = None,\n",
    "                              other_metrics: Optional[Dict] = None):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive evaluation summary.\n",
    "    \"\"\"\n",
    "    print(\"\" + \"=\"*80)\n",
    "    print(\"SPECIESNET EVALUATION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Dataset overview\n",
    "    total_samples = len(evaluation_data)\n",
    "    successful_samples = len(evaluation_data[~evaluation_data['failed']])\n",
    "    failed_samples = len(evaluation_data[evaluation_data['failed']])\n",
    "    \n",
    "    print(f\"Dataset Overview:\")\n",
    "    print(f\"- Total samples: {total_samples}\")\n",
    "    print(f\"- Successful predictions: {successful_samples} ({successful_samples/total_samples*100:.1f}%)\")\n",
    "    print(f\"- Failed predictions: {failed_samples} ({failed_samples/total_samples*100:.1f}%)\")\n",
    "    \n",
    "    # Failure analysis\n",
    "    if failed_samples > 0:\n",
    "        failure_types = Counter()\n",
    "        for _, row in evaluation_data[evaluation_data['failed']].iterrows():\n",
    "            for failure in row['failure_types']:\n",
    "                failure_types[failure] += 1\n",
    "        \n",
    "        print(f\"Failure Analysis:\")\n",
    "        for failure_type, count in failure_types.items():\n",
    "            print(f\"- {failure_type}: {count} ({count/failed_samples*100:.1f}% of failures)\")\n",
    "    \n",
    "    # Species-level performance\n",
    "    if species_metrics:\n",
    "        print(f\"Species-Level Performance:\")\n",
    "        print(f\"- Accuracy: {species_metrics['accuracy']:.4f}\")\n",
    "        print(f\"- Precision (weighted): {species_metrics['precision']:.4f}\")\n",
    "        print(f\"- Recall (weighted): {species_metrics['recall']:.4f}\")\n",
    "        print(f\"- F1-Score (weighted): {species_metrics['f1_score']:.4f}\")\n",
    "        print(f\"- Number of unique species: {species_metrics['num_classes']}\")\n",
    "    \n",
    "    # Hierarchical accuracy\n",
    "    if hierarchical_acc:\n",
    "        print(f\"Hierarchical Accuracy:\")\n",
    "        for level, acc in hierarchical_acc.items():\n",
    "            print(f\"- {level.capitalize()}: {acc:.4f}\")\n",
    "    \n",
    "    # Other taxonomic levels\n",
    "    if other_metrics:\n",
    "        print(f\"Other Taxonomic Levels:\")\n",
    "        for level, metrics in other_metrics.items():\n",
    "            print(f\"- {level.capitalize()}: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Confidence statistics\n",
    "    successful_data = evaluation_data[~evaluation_data['failed']]\n",
    "    if len(successful_data) > 0:\n",
    "        confidence_stats = successful_data['prediction_score'].describe()\n",
    "        print(f\"Confidence Score Statistics:\")\n",
    "        print(f\"- Mean: {confidence_stats['mean']:.4f}\")\n",
    "        print(f\"- Median: {confidence_stats['50%']:.4f}\")\n",
    "        print(f\"- Std: {confidence_stats['std']:.4f}\")\n",
    "        print(f\"- Min: {confidence_stats['min']:.4f}\")\n",
    "        print(f\"- Max: {confidence_stats['max']:.4f}\")\n",
    "    \n",
    "    # Top predictions\n",
    "    if len(successful_data) > 0:\n",
    "        top_predictions = successful_data['prediction'].value_counts().head(10)\n",
    "        print(f\"Top 10 Most Common Predictions:\")\n",
    "        for i, (pred, count) in enumerate(top_predictions.items()):\n",
    "            print(f\"{i+1:2d}. {pred}: {count} ({count/len(successful_data)*100:.1f}%)\")\n",
    "\n",
    "def export_results(evaluation_data: pd.DataFrame, \n",
    "                  species_metrics: Optional[Dict] = None,\n",
    "                  output_dir: str = \".\"):\n",
    "    \"\"\"\n",
    "    Export evaluation results to files.\n",
    "    \"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Export detailed results\n",
    "    results_file = output_path / \"speciesnet_evaluation_results.csv\"\n",
    "    evaluation_data.to_csv(results_file, index=False)\n",
    "    print(f\"Detailed results exported to: {results_file}\")\n",
    "    \n",
    "    # Export summary metrics\n",
    "    if species_metrics:\n",
    "        metrics_file = output_path / \"speciesnet_metrics_summary.json\"\n",
    "        \n",
    "        # Prepare metrics for JSON serialization\n",
    "        exportable_metrics = {\n",
    "            'taxonomic_level': species_metrics['taxonomic_level'],\n",
    "            'accuracy': float(species_metrics['accuracy']),\n",
    "            'precision': float(species_metrics['precision']),\n",
    "            'recall': float(species_metrics['recall']),\n",
    "            'f1_score': float(species_metrics['f1_score']),\n",
    "            'num_samples': int(species_metrics['num_samples']),\n",
    "            'num_classes': int(species_metrics['num_classes'])\n",
    "        }\n",
    "        \n",
    "        with open(metrics_file, 'w') as f:\n",
    "            json.dump(exportable_metrics, f, indent=2)\n",
    "        \n",
    "        print(f\"Metrics summary exported to: {metrics_file}\")\n",
    "    \n",
    "    # Export per-class metrics\n",
    "    if species_metrics and 'per_class_metrics' in species_metrics:\n",
    "        per_class_file = output_path / \"speciesnet_per_class_metrics.json\"\n",
    "        \n",
    "        with open(per_class_file, 'w') as f:\n",
    "            json.dump(species_metrics['per_class_metrics'], f, indent=2)\n",
    "        \n",
    "        print(f\"Per-class metrics exported to: {per_class_file}\")\n",
    "\n",
    "# Generate summary and export results if data is available\n",
    "if 'evaluation_data' in locals() and len(evaluation_data) > 0:\n",
    "    print(\"Generating evaluation summary...\")\n",
    "    \n",
    "    # Generate comprehensive summary\n",
    "    generate_evaluation_summary(\n",
    "        evaluation_data,\n",
    "        species_metrics if 'species_metrics' in locals() else None,\n",
    "        hierarchical_acc if 'hierarchical_acc' in locals() else None,\n",
    "        other_metrics if 'other_metrics' in locals() else None\n",
    "    )\n",
    "    \n",
    "    # Export results\n",
    "    try:\n",
    "        export_results(\n",
    "            evaluation_data,\n",
    "            species_metrics if 'species_metrics' in locals() else None,\n",
    "            output_dir=\".\"\n",
    "        )\n",
    "        print(\"All results exported successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "    \n",
    "    print(\"\" + \"=\"*80)\n",
    "    print(\"EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Next steps:\")\n",
    "    print(\"1. Review the exported CSV file for detailed per-image results\")\n",
    "    print(\"2. Use the JSON files for integration with other analysis tools\")\n",
    "    print(\"3. Consider fine-tuning or domain adaptation if accuracy is low\")\n",
    "    print(\"4. Explore taxonomic hierarchy-aware evaluation metrics\")\n",
    "    \n",
    "else:\n",
    "    print(\"No evaluation data available for summary generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete evaluation pipeline summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPECIESNET WILDLIFE CLASSIFICATION EVALUATION COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if 'evaluation_data' in locals() and len(evaluation_data) > 0:\n",
    "    print(\"\\nEVALUATION SUMMARY:\")\n",
    "    successful_preds = len(evaluation_data[~evaluation_data['failed']])\n",
    "    total_preds = len(evaluation_data)\n",
    "    success_rate = successful_preds / total_preds * 100 if total_preds > 0 else 0\n",
    "    \n",
    "    print(f\"  - Total images processed: {total_preds}\")\n",
    "    print(f\"  - Successful predictions: {successful_preds} ({success_rate:.1f}%)\")\n",
    "    \n",
    "    if 'species_metrics' in locals() and species_metrics:\n",
    "        print(f\"  - Species-level accuracy: {species_metrics['accuracy']:.4f}\")\n",
    "        print(f\"  - Weighted F1-score: {species_metrics['f1_score']:.4f}\")\n",
    "        print(f\"  - Number of species: {species_metrics['num_classes']}\")\n",
    "    \n",
    "    if 'hierarchical_acc' in locals() and hierarchical_acc:\n",
    "        print(f\"  - Genus-level accuracy: {hierarchical_acc.get('genus', 0):.4f}\")\n",
    "        print(f\"  - Family-level accuracy: {hierarchical_acc.get('family', 0):.4f}\")\n",
    "    \n",
    "    print(f\"\\nOutput files generated:\")\n",
    "    print(\"  - speciesnet_evaluation_results.csv (detailed per-image results)\")\n",
    "    print(\"  - speciesnet_metrics_summary.json (overall performance metrics)\")\n",
    "    print(\"  - speciesnet_per_class_metrics.json (species-specific performance)\")\n",
    "\n",
    "print(f\"\\nThis completes the downstream evaluation for wildlife species classification.\")\n",
    "print(\"Use the generated metrics and visualizations to assess restoration model quality\")  \n",
    "print(\"and determine impact on ecological monitoring applications.\")\n",
    "\n",
    "print(\"\\nNext steps for analysis:\")\n",
    "print(\"1. Review hierarchical accuracy to understand taxonomic-level performance\")\n",
    "print(\"2. Examine confusion matrices to identify commonly confused species pairs\")\n",
    "print(\"3. Analyze confidence distributions to set appropriate deployment thresholds\")\n",
    "print(\"4. Use sample image analysis to validate model behavior on challenging cases\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
