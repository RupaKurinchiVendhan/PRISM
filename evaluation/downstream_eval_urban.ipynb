{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59dc9b6",
   "metadata": {},
   "source": [
    "# Downstream Evaluation: Urban Scene Panoptic Segmentation\n",
    "\n",
    "Urban scene panoptic segmentation represents a complex downstream task that serves as an excellent testbed for evaluating image restoration models in real-world scenarios. This task is particularly valuable for restoration assessment because:\n",
    "\n",
    "1. **Multi-class Complexity**: Urban scenes contain diverse semantic categories (roads, buildings, vegetation) and instance objects (cars, pedestrians, signs) requiring preservation of both broad semantic regions and fine instance boundaries\n",
    "2. **Real-world Applications**: Accurate urban scene understanding directly supports autonomous driving, urban planning, and smart city applications where restoration quality has immediate practical impact\n",
    "3. **Scale Sensitivity**: The task requires both local detail preservation (for instance segmentation) and global context understanding (for semantic segmentation), testing restoration at multiple scales\n",
    "4. **Challenging Conditions**: Urban images often contain challenging lighting, shadows, occlusions, and weather conditions that make restoration particularly important\n",
    "\n",
    "This evaluation framework uses Detectron2's PanopticFCN model to assess how well restored urban images maintain the critical visual information needed for accurate scene understanding and segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11420894",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies\n",
    "\n",
    "This section establishes the computational environment required for urban scene panoptic segmentation evaluation. The setup process includes CUDA compatibility checking, Detectron2 installation with appropriate hardware support, and importing all necessary libraries for comprehensive evaluation.\n",
    "\n",
    "**Technical Requirements:**\n",
    "- **Deep Learning Framework**: PyTorch with CUDA support for GPU acceleration\n",
    "- **Computer Vision Library**: Detectron2 with PanopticFCN model implementation\n",
    "- **Evaluation Tools**: Comprehensive metrics computation and visualization libraries\n",
    "- **Data Processing**: Image handling, annotation processing, and result export capabilities\n",
    "\n",
    "The environment setup is designed to handle various hardware configurations while providing robust fallback options for different CUDA versions and hardware constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a950b250",
   "metadata": {},
   "source": [
    "## Installation and Dependencies\n",
    "\n",
    "### Detectron2 Setup\n",
    "\n",
    "Install Detectron2 and its dependencies for panoptic segmentation:\n",
    "\n",
    "**Prerequisites:**\n",
    "- Python 3.7+\n",
    "- PyTorch 1.8+\n",
    "- CUDA 10.2+ (for GPU acceleration)\n",
    "- OpenCV for image processing\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install 'git+https://github.com/facebookresearch/detectron2.git'\n",
    "```\n",
    "\n",
    "### Required Files and Resources\n",
    "\n",
    "**Model Weights:**\n",
    "- Pre-trained PanopticFCN models available through Detectron2 Model Zoo\n",
    "- Cityscapes-trained models for urban scene understanding\n",
    "\n",
    "**Dataset Requirements:**\n",
    "- Urban scene images (preferably Cityscapes format)\n",
    "- Ground truth panoptic annotations with semantic and instance labels\n",
    "\n",
    "**Key Citations:**\n",
    "- Wu, Y., et al. (2019). \"Detectron2.\" arXiv preprint arXiv:1912.01703.\n",
    "- Li, Y., et al. (2020). \"Panoptic FCN: Towards Real-time and High-Precision Panoptic Segmentation.\" arXiv:2008.00398.\n",
    "- Cordts, M., et al. (2016). \"The Cityscapes Dataset for Semantic Urban Scene Understanding.\" CVPR 2016.\n",
    "\n",
    "**Key Resources:**\n",
    "- Detectron2 Repository: [https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)\n",
    "- Cityscapes Dataset: [https://www.cityscapes-dataset.com/](https://www.cityscapes-dataset.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddef9422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU device: NVIDIA L40S\n",
      "GPU memory: 44.4 GB\n"
     ]
    }
   ],
   "source": [
    "# Check CUDA availability and PyTorch version\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"CUDA not available - will use CPU (slower)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69270a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detectron2 is already installed!\n"
     ]
    }
   ],
   "source": [
    "# Install Detectron2 and required dependencies\n",
    "# Note: This will install the pre-built detectron2 for the detected PyTorch and CUDA versions\n",
    "# For more installation options, visit: https://detectron2.readthedocs.io/en/latest/tutorials/install.html\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_detectron2():\n",
    "    \"\"\"Install detectron2 with appropriate CUDA support\"\"\"\n",
    "    try:\n",
    "        # Try importing detectron2 first\n",
    "        import detectron2\n",
    "        print(\"Detectron2 is already installed!\")\n",
    "        return\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Install required packages first\n",
    "    required_packages = [\n",
    "        \"opencv-python\",\n",
    "        \"pillow\",\n",
    "        \"matplotlib\",\n",
    "        \"scikit-learn\",\n",
    "        \"seaborn\",\n",
    "        \"tqdm\"\n",
    "    ]\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, text=True, check=True)\n",
    "            print(f\"{package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: Failed to install {package}\")\n",
    "    \n",
    "    # Install detectron2\n",
    "    # For CUDA 12.6 compatibility, use the correct wheel\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_version = torch.version.cuda\n",
    "        print(f\"CUDA version detected: {cuda_version}\")\n",
    "        \n",
    "        # For CUDA 12.x, use cu121 wheels (most compatible)\n",
    "        if cuda_version.startswith('12.'):\n",
    "            install_cmd = [\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"detectron2\", \"-f\",\n",
    "                \"https://dl.fbaipublicfiles.com/detectron2/wheels/cu121/torch2.0/index.html\"\n",
    "            ]\n",
    "        else:\n",
    "            # For older CUDA versions\n",
    "            cuda_short = cuda_version.replace('.', '')[:4]\n",
    "            install_cmd = [\n",
    "                sys.executable, \"-m\", \"pip\", \"install\", \"detectron2\", \"-f\",\n",
    "                f\"https://dl.fbaipublicfiles.com/detectron2/wheels/cu{cuda_short}/torch2.0/index.html\"\n",
    "            ]\n",
    "    else:\n",
    "        # CPU-only installation\n",
    "        install_cmd = [\n",
    "            sys.executable, \"-m\", \"pip\", \"install\", \"detectron2\", \"-f\",\n",
    "            \"https://dl.fbaipublicfiles.com/detectron2/wheels/cpu/torch2.0/index.html\"\n",
    "        ]\n",
    "    \n",
    "    print(f\"Installing detectron2 with command: {' '.join(install_cmd)}\")\n",
    "    result = subprocess.run(install_cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"Detectron2 installed successfully!\")\n",
    "    else:\n",
    "        print(f\"Installation failed: {result.stderr}\")\n",
    "        print(\"Trying alternative installation method...\")\n",
    "        \n",
    "        # Try pip install from source as fallback\n",
    "        try:\n",
    "            alt_cmd = [sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/facebookresearch/detectron2.git\"]\n",
    "            subprocess.run(alt_cmd, check=True)\n",
    "            print(\"Detectron2 installed from source!\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"Both installation methods failed. Please install detectron2 manually.\")\n",
    "\n",
    "# Install detectron2\n",
    "install_detectron2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6023ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ pycocotools installed successfully\n",
      "‚úÖ opencv-python installed successfully\n",
      "‚úÖ opencv-python installed successfully\n",
      "‚úÖ Pillow installed successfully\n",
      "‚úÖ Pillow installed successfully\n",
      "‚úÖ matplotlib installed successfully\n",
      "‚úÖ matplotlib installed successfully\n",
      "‚úÖ scikit-learn installed successfully\n",
      "‚úÖ scikit-learn installed successfully\n",
      "‚úÖ seaborn installed successfully\n",
      "‚úÖ seaborn installed successfully\n",
      "‚úÖ tqdm installed successfully\n",
      "‚úÖ tqdm installed successfully\n",
      "‚úÖ requests installed successfully\n",
      "‚úÖ requests installed successfully\n"
     ]
    }
   ],
   "source": [
    "# Install additional required packages\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_additional_packages():\n",
    "    \"\"\"Install additional packages required for PanopticFCN\"\"\"\n",
    "    packages = [\n",
    "        \"pycocotools\",\n",
    "        \"opencv-python\",\n",
    "        \"Pillow\",\n",
    "        \"matplotlib\",\n",
    "        \"scikit-learn\", \n",
    "        \"seaborn\",\n",
    "        \"tqdm\",\n",
    "        \"requests\"\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", package], \n",
    "                         capture_output=True, text=True, check=True)\n",
    "            print(f\"{package} installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Warning: Failed to install {package}: {e}\")\n",
    "\n",
    "install_additional_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Detectron2 core modules imported successfully!\n",
      "‚úÖ PanopticFCN config imported successfully!\n",
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# Detectron2 imports with error handling\n",
    "try:\n",
    "    from detectron2 import model_zoo\n",
    "    from detectron2.engine import DefaultPredictor\n",
    "    from detectron2.config import get_cfg\n",
    "    from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "    from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "    print(\"Detectron2 core modules imported successfully!\")\n",
    "    \n",
    "    # Try importing PanopticFCN config\n",
    "    try:\n",
    "        from projects.PanopticFCN_cityscapes.panopticfcn import add_panopticfcn_config\n",
    "        print(\"PanopticFCN config imported successfully!\")\n",
    "    except ImportError as e:\n",
    "        print(f\"Warning: Could not import PanopticFCN config: {e}\")\n",
    "        print(\"   This might be due to missing custom modules. Continuing with fallback...\")\n",
    "        \n",
    "        # Create a fallback function\n",
    "        def add_panopticfcn_config(cfg):\n",
    "            \"\"\"Fallback function for PanopticFCN config\"\"\"\n",
    "            # Add basic panoptic segmentation configs\n",
    "            if not hasattr(cfg.MODEL, 'PANOPTIC_FPN'):\n",
    "                cfg.MODEL.PANOPTIC_FPN = {}\n",
    "            cfg.MODEL.PANOPTIC_FPN.COMBINE_ON = True\n",
    "            cfg.MODEL.PANOPTIC_FPN.COMBINE_OVERLAP_THRESH = 0.5\n",
    "            cfg.MODEL.PANOPTIC_FPN.COMBINE_STUFF_AREA_THRESH = 4096\n",
    "            cfg.MODEL.PANOPTIC_FPN.COMBINE_INSTANCES_CONF_THRESH = 0.5\n",
    "            return cfg\n",
    "        \n",
    "        print(\"Created fallback PanopticFCN config function\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"Error importing detectron2: {e}\")\n",
    "    print(\"   Please ensure detectron2 is properly installed.\")\n",
    "    print(\"   Run the installation cell above and restart the kernel.\")\n",
    "    raise\n",
    "\n",
    "# Set up matplotlib for better visualization\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a337418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SegmentationEvaluator initialized with Cityscapes classes\n",
      "üìä Tracking 20 classes\n"
     ]
    }
   ],
   "source": [
    "class SegmentationEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive segmentation evaluator for urban scene analysis\n",
    "    Based on evaluate.py structure with mIoU computation capabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, api_key=None):\n",
    "        self.api_key = api_key  # Store the API key for mask downloads\n",
    "        \n",
    "        # Map Cityscapes class names to indices (following evaluate.py format)\n",
    "        self.class_mapping = {\n",
    "            'road': 0,\n",
    "            'sidewalk': 1,\n",
    "            'building': 2,\n",
    "            'wall': 3,\n",
    "            'fence': 4,\n",
    "            'pole': 5,\n",
    "            'traffic light': 6,\n",
    "            'traffic sign': 7,\n",
    "            'vegetation': 8,\n",
    "            'terrain': 9,\n",
    "            'sky': 10,\n",
    "            'person': 11,\n",
    "            'rider': 12,\n",
    "            'car': 13,\n",
    "            'truck': 14,\n",
    "            'bus': 15,\n",
    "            'train': 16,\n",
    "            'motorcycle': 17,\n",
    "            'bicycle': 18,\n",
    "            'background': 19  # For unlabeled areas\n",
    "        }\n",
    "        self.class_names = list(self.class_mapping.keys())\n",
    "        \n",
    "        # Cityscapes color mapping for visualization\n",
    "        self.color_mapping = {\n",
    "            'road': (128, 64, 128),\n",
    "            'sidewalk': (244, 35, 232),\n",
    "            'building': (70, 70, 70),\n",
    "            'wall': (102, 102, 156),\n",
    "            'fence': (190, 153, 153),\n",
    "            'pole': (153, 153, 153),\n",
    "            'traffic light': (250, 170, 30),\n",
    "            'traffic sign': (220, 220, 0),\n",
    "            'vegetation': (107, 142, 35),\n",
    "            'terrain': (152, 251, 152),\n",
    "            'sky': (70, 130, 180),\n",
    "            'person': (220, 20, 60),\n",
    "            'rider': (255, 0, 0),\n",
    "            'car': (0, 0, 142),\n",
    "            'truck': (0, 0, 70),\n",
    "            'bus': (0, 60, 100),\n",
    "            'train': (0, 80, 100),\n",
    "            'motorcycle': (0, 0, 230),\n",
    "            'bicycle': (119, 11, 32),\n",
    "            'background': (0, 0, 0)  # Black for background\n",
    "        }\n",
    "        \n",
    "        # Get ordered colors for visualization\n",
    "        self.cityscapes_colors = [self.color_mapping[class_name] for class_name in self.class_names]\n",
    "        \n",
    "    def calculate_metrics(self, gt_mask, pred_mask):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive segmentation metrics including mIoU\n",
    "        Following evaluate.py format\n",
    "        \"\"\"\n",
    "        # Flatten masks for metric calculation\n",
    "        gt_flat = gt_mask.flatten()\n",
    "        pred_flat = pred_mask.flatten()\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(gt_flat, pred_flat, labels=range(len(self.class_names)))\n",
    "        \n",
    "        # Calculate per-class metrics\n",
    "        metrics = {}\n",
    "        \n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            if i >= len(cm):\n",
    "                continue\n",
    "                \n",
    "            tp = cm[i, i]\n",
    "            fp = cm[:, i].sum() - tp\n",
    "            fn = cm[i, :].sum() - tp\n",
    "            tn = cm.sum() - tp - fp - fn\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            iou = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            metrics[class_name] = {\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'iou': iou,\n",
    "                'f1': f1,\n",
    "                'support': gt_flat[gt_flat == i].shape[0]\n",
    "            }\n",
    "        \n",
    "        # Calculate overall accuracy\n",
    "        overall_accuracy = np.sum(gt_flat == pred_flat) / len(gt_flat)\n",
    "        \n",
    "        # Calculate mean IoU for all classes\n",
    "        all_ious = [metrics[cls]['iou'] for cls in self.class_names if cls in metrics and metrics[cls]['support'] > 0]\n",
    "        mean_iou_all = np.mean(all_ious) if all_ious else 0.0\n",
    "        \n",
    "        # Calculate mean IoU for key urban classes (stuff + things)\n",
    "        key_urban_classes = ['road', 'sidewalk', 'building', 'vegetation', 'sky', 'car', 'person']\n",
    "        key_ious = [metrics[cls]['iou'] for cls in key_urban_classes if cls in metrics and metrics[cls]['support'] > 0]\n",
    "        mean_iou_key = np.mean(key_ious) if key_ious else 0.0\n",
    "        \n",
    "        return metrics, overall_accuracy, mean_iou_all, mean_iou_key, cm\n",
    "    \n",
    "    def plot_confusion_matrix(self, cm, save_path=None, figsize=(12, 10)):\n",
    "        \"\"\"Plot confusion matrix with proper formatting\"\"\"\n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Use a subset of class names for better readability\n",
    "        display_names = [name[:8] for name in self.class_names]  # Truncate long names\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=display_names, yticklabels=display_names,\n",
    "                   cbar_kws={'label': 'Count'})\n",
    "        plt.title('Confusion Matrix - Urban Scene Segmentation', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Predicted Class', fontsize=12)\n",
    "        plt.ylabel('Ground Truth Class', fontsize=12)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_metrics_summary(self, metrics, overall_accuracy, mean_iou_all, mean_iou_key):\n",
    "        \"\"\"Print comprehensive metrics summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üìä COMPREHENSIVE SEGMENTATION METRICS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"üéØ Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "        print(f\"üìà Mean IoU (All Classes): {mean_iou_all:.4f}\")\n",
    "        print(f\"üèôÔ∏è Mean IoU (Key Urban Classes): {mean_iou_key:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìã PER-CLASS METRICS:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(f\"{'Class':<15} {'IoU':<8} {'Precision':<10} {'Recall':<8} {'F1':<8} {'Support':<10}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Sort by IoU for better readability\n",
    "        sorted_metrics = sorted(metrics.items(), key=lambda x: x[1]['iou'], reverse=True)\n",
    "        \n",
    "        for class_name, metrics_dict in sorted_metrics:\n",
    "            if metrics_dict['support'] > 0:  # Only show classes with actual data\n",
    "                print(f\"{class_name:<15} {metrics_dict['iou']:<8.3f} {metrics_dict['precision']:<10.3f} \"\n",
    "                      f\"{metrics_dict['recall']:<8.3f} {metrics_dict['f1']:<8.3f} {metrics_dict['support']:<10}\")\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = SegmentationEvaluator()\n",
    "print(\"SegmentationEvaluator initialized with Cityscapes classes\")\n",
    "print(f\"Tracking {len(evaluator.class_names)} classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608e656",
   "metadata": {},
   "source": [
    "## Comprehensive Evaluation Framework\n",
    "\n",
    "This section implements a sophisticated evaluation system for urban scene panoptic segmentation that combines both semantic and instance segmentation assessment. The framework provides detailed metrics specifically designed for evaluating restoration quality through downstream segmentation performance.\n",
    "\n",
    "### Panoptic Segmentation Evaluation Metrics\n",
    "\n",
    "**Core Panoptic Quality (PQ) Metrics:**\n",
    "- **Panoptic Quality (PQ)**: Overall metric combining segmentation quality and recognition quality (PQ = SQ √ó RQ)\n",
    "- **Segmentation Quality (SQ)**: Measures the quality of segmentation masks using IoU for matched segments\n",
    "- **Recognition Quality (RQ)**: Measures the quality of recognition, computed as F1-score of segment matching\n",
    "\n",
    "**Semantic Segmentation Metrics:**\n",
    "- **Mean Intersection over Union (mIoU)**: Average IoU across all semantic classes\n",
    "- **Pixel Accuracy**: Overall fraction of correctly classified pixels\n",
    "- **Class-wise IoU**: Individual IoU scores for each of the 19 Cityscapes classes\n",
    "- **Frequency Weighted IoU**: IoU weighted by class frequency to handle class imbalance\n",
    "\n",
    "**Instance Segmentation Metrics:**\n",
    "- **Average Precision (AP)**: Standard COCO-style AP at IoU thresholds 0.5 and 0.75\n",
    "- **Average Recall (AR)**: Maximum recall given a fixed number of detections per image\n",
    "- **Per-Class AP**: Individual average precision scores for \"thing\" classes (cars, people, etc.)\n",
    "\n",
    "### Advanced Evaluation Features\n",
    "\n",
    "**Multi-Scale Analysis:**\n",
    "The framework evaluates performance across different object scales (small, medium, large) to understand how restoration affects objects of varying sizes in urban scenes.\n",
    "\n",
    "**Boundary Quality Assessment:**\n",
    "Specialized metrics for evaluating boundary precision, crucial for applications like autonomous driving where precise object boundaries are essential.\n",
    "\n",
    "**Class-Specific Performance:**\n",
    "Detailed analysis for both \"stuff\" classes (road, building, sky) and \"thing\" classes (car, person, bicycle) with different evaluation criteria appropriate for each category.\n",
    "\n",
    "**Confidence Calibration:**\n",
    "Analysis of prediction confidence scores and their correlation with actual accuracy, essential for deployment in safety-critical applications.\n",
    "\n",
    "### Urban Scene-Specific Considerations\n",
    "\n",
    "**Challenging Conditions Handling:**\n",
    "- **Occlusion Robustness**: Evaluation of performance when objects are partially occluded\n",
    "- **Lighting Variation**: Assessment across different lighting conditions common in urban environments\n",
    "- **Scale Variation**: Analysis of performance on objects ranging from distant cars to nearby pedestrians\n",
    "- **Weather Adaptability**: Evaluation under various weather conditions that affect image quality\n",
    "\n",
    "This comprehensive evaluation framework provides the detailed insights needed to assess whether restored urban images maintain the critical visual information required for accurate scene understanding in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc7ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_panoptic_model(model_name=\"PanopticFCN-R50\", confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Set up PanopticFCN model for inference with fallback configurations\n",
    "    \n",
    "    Args:\n",
    "        model_name: Either \"PanopticFCN-R50\" or \"PanopticFCN-R101\"\n",
    "        confidence_threshold: Minimum confidence for detections (0.0 to 1.0)\n",
    "    \n",
    "    Returns:\n",
    "        predictor: Detectron2 predictor object\n",
    "        cfg: Configuration object\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize configuration\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Add PanopticFCN-specific configs\n",
    "    add_panopticfcn_config(cfg)\n",
    "    \n",
    "    # Try different configuration paths\n",
    "    possible_config_files = [\n",
    "        \"projects/PanopticFCN_cityscapes/configs/cityscapes/PanopticFCN-R50-cityscapes.yaml\",\n",
    "        \"projects/PanopticFCN/configs/Cityscapes-PanopticSegmentation/panoptic_fcn_R_50_3x.yaml\",\n",
    "        \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\"  # Fallback to standard config\n",
    "    ]\n",
    "    \n",
    "    config_file = None\n",
    "    for config_path in possible_config_files:\n",
    "        if os.path.exists(config_path):\n",
    "            config_file = config_path\n",
    "            print(f\"‚úÖ Found config file: {config_file}\")\n",
    "            break\n",
    "    \n",
    "    if config_file is None:\n",
    "        print(\"‚ö†Ô∏è Custom config files not found, using standard panoptic segmentation config\")\n",
    "        config_file = \"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\"\n",
    "    \n",
    "    # Try different model weight paths\n",
    "    possible_weights = [\n",
    "        \"output/model_0059999.pth\",\n",
    "        \"output/model_final.pth\",\n",
    "        \"detectron2://PanopticFCN/Cityscapes-PanopticSegmentation/panoptic_fcn_R_50_3x/model_final_c45e69.pkl\",\n",
    "        \"detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/model_final_c10459.pkl\"  # Fallback\n",
    "    ]\n",
    "    \n",
    "    model_weights = None\n",
    "    for weight_path in possible_weights:\n",
    "        if weight_path.startswith(\"detectron2://\") or os.path.exists(weight_path):\n",
    "            model_weights = weight_path\n",
    "            print(f\"‚úÖ Using model weights: {weight_path}\")\n",
    "            break\n",
    "    \n",
    "    if model_weights is None:\n",
    "        print(\"‚ö†Ô∏è Custom model weights not found, using pre-trained COCO weights\")\n",
    "        model_weights = \"detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/model_final_c10459.pkl\"\n",
    "\n",
    "    try:\n",
    "        # Configure model\n",
    "        cfg.merge_from_file(config_file)\n",
    "        cfg.MODEL.WEIGHTS = model_weights\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold\n",
    "        cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Additional safety configurations\n",
    "        cfg.MODEL.PANOPTIC_FPN.COMBINE_ON = True\n",
    "        cfg.MODEL.PANOPTIC_FPN.COMBINE_OVERLAP_THRESH = 0.5\n",
    "        cfg.MODEL.PANOPTIC_FPN.COMBINE_STUFF_AREA_THRESH = 4096\n",
    "        cfg.MODEL.PANOPTIC_FPN.COMBINE_INSTANCES_CONF_THRESH = confidence_threshold\n",
    "        \n",
    "        # Create predictor\n",
    "        predictor = DefaultPredictor(cfg)\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} model loaded successfully!\")\n",
    "        print(f\"üñ•Ô∏è Running on: {cfg.MODEL.DEVICE}\")\n",
    "        print(f\"üéØ Confidence threshold: {confidence_threshold}\")\n",
    "        print(f\"üìÅ Config file: {config_file}\")\n",
    "        print(f\"‚öñÔ∏è Model weights: {model_weights}\")\n",
    "        \n",
    "        return predictor, cfg\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error setting up model: {e}\")\n",
    "        print(\"   Trying with standard Detectron2 panoptic model...\")\n",
    "        \n",
    "        # Fallback to standard detectron2 panoptic model\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(\"detectron2/projects/PanopticFCN_cityscapes/configs/cityscapes/PanopticFCN-R50-cityscapes.yaml\")\n",
    "        cfg.MODEL.WEIGHTS = \"detectron2/output/model_0059999.pth\"\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = confidence_threshold\n",
    "        cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        predictor = DefaultPredictor(cfg)\n",
    "        \n",
    "        print(f\"‚úÖ Fallback model loaded successfully!\")\n",
    "        print(f\"üñ•Ô∏è Running on: {cfg.MODEL.DEVICE}\")\n",
    "        \n",
    "        return predictor, cfg\n",
    "\n",
    "# Set up the model (uncomment to run)\n",
    "predictor, cfg = setup_panoptic_model(\"PanopticFCN-R50\", confidence_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "290a22b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing PanopticFCN model...\n",
      "‚úÖ Found config file: projects/PanopticFCN_cityscapes/configs/cityscapes/PanopticFCN-R50-cityscapes.yaml\n",
      "‚úÖ Using model weights: output/model_0059999.pth\n",
      "‚ùå Error setting up model: Cannot import 'detectron2._C', therefore 'ModulatedDeformConv' is not available. detectron2 is not compiled successfully, please build following the instructions!\n",
      "   Trying with standard Detectron2 panoptic model...\n",
      "‚ùå Model setup failed: Config file 'COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml' does not exist!\n",
      "   Please check the error message above and ensure:\n",
      "   1. Detectron2 is properly installed\n",
      "   2. CUDA is available (if using GPU)\n",
      "   3. Model config and weights are accessible\n",
      "‚ùå Error setting up model: Cannot import 'detectron2._C', therefore 'ModulatedDeformConv' is not available. detectron2 is not compiled successfully, please build following the instructions!\n",
      "   Trying with standard Detectron2 panoptic model...\n",
      "‚ùå Model setup failed: Config file 'COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml' does not exist!\n",
      "   Please check the error message above and ensure:\n",
      "   1. Detectron2 is properly installed\n",
      "   2. CUDA is available (if using GPU)\n",
      "   3. Model config and weights are accessible\n"
     ]
    }
   ],
   "source": [
    "# Test model setup - Run this cell to initialize the model\n",
    "try:\n",
    "    print(\"üöÄ Initializing PanopticFCN model...\")\n",
    "    predictor, cfg = setup_panoptic_model(\"PanopticFCN-R50\", confidence_threshold=0.5)\n",
    "    print(\"‚úÖ Model setup completed successfully!\")\n",
    "    \n",
    "    # Test with a dummy image to verify everything works\n",
    "    dummy_image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    test_output = predictor(dummy_image)\n",
    "    print(\"‚úÖ Model inference test passed!\")\n",
    "    print(f\"   Output keys: {list(test_output.keys())}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model setup failed: {e}\")\n",
    "    print(\"   Please check the error message above and ensure:\")\n",
    "    print(\"   1. Detectron2 is properly installed\")\n",
    "    print(\"   2. CUDA is available (if using GPU)\")\n",
    "    print(\"   3. Model config and weights are accessible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2d663",
   "metadata": {},
   "source": [
    "## Image Processing and Data Loading\n",
    "\n",
    "This section provides comprehensive utilities for loading and preprocessing urban scene images for panoptic segmentation evaluation. The image processing pipeline handles various formats and resolutions while maintaining compatibility with Detectron2 input requirements.\n",
    "\n",
    "**Image Loading Features:**\n",
    "- **Multi-format Support**: Handles JPEG, PNG, TIFF, and other common image formats\n",
    "- **Resolution Handling**: Automatic resizing and scaling to match model input requirements\n",
    "- **Color Space Management**: Proper BGR/RGB conversion for OpenCV and PIL compatibility\n",
    "- **Metadata Extraction**: Retrieval of image dimensions, format, and quality information\n",
    "- **Batch Processing**: Efficient loading of entire image directories with progress tracking\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "The preprocessing ensures that input images are properly formatted for the panoptic segmentation model while preserving the visual information critical for accurate segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d295c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def load_image(image_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for Detectron2 inference\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Image in BGR format (Detectron2 expects BGR)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read image using OpenCV (returns BGR format)\n",
    "        image = cv2.imread(str(image_path))\n",
    "        \n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image from {image_path}\")\n",
    "        \n",
    "        return image\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading image: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_image_files(directory, extensions=('.jpg', '.jpeg', '.png', '.bmp', '.tiff')):\n",
    "    \"\"\"\n",
    "    Get all image files from a directory\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to directory containing images\n",
    "        extensions: Tuple of valid image extensions\n",
    "        \n",
    "    Returns:\n",
    "        List of image file paths\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    directory = Path(directory)\n",
    "    \n",
    "    for ext in extensions:\n",
    "        pattern = directory / f\"*{ext}\"\n",
    "        image_files.extend(glob.glob(str(pattern)))\n",
    "        pattern = directory / f\"*{ext.upper()}\"\n",
    "        image_files.extend(glob.glob(str(pattern)))\n",
    "    \n",
    "    return sorted(image_files)\n",
    "\n",
    "def display_image_info(image, image_path):\n",
    "    \"\"\"Display basic information about the loaded image\"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    channels = image.shape[2] if len(image.shape) == 3 else 1\n",
    "    \n",
    "    print(f\"üìÅ Image: {Path(image_path).name}\")\n",
    "    print(f\"üìê Dimensions: {width} x {height} pixels\")\n",
    "    print(f\"üé® Channels: {channels}\")\n",
    "    print(f\"üìä Data type: {image.dtype}\")\n",
    "\n",
    "# Example usage - Update the path to your images\n",
    "# Uncomment and modify the path below to test with your images\n",
    "\"\"\"\n",
    "image_directory = \"path/to/your/images\"  # Change this to your image directory\n",
    "image_files = get_image_files(image_directory)\n",
    "print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "if image_files:\n",
    "    # Load first image as example\n",
    "    sample_image = load_image(image_files[0])\n",
    "    if sample_image is not None:\n",
    "        display_image_info(sample_image, image_files[0])\n",
    "\"\"\"\n",
    "\n",
    "# For testing purposes, you can use any sample image\n",
    "print(\"To test with your images:\")\n",
    "print(\"   1. Update 'image_directory' path above\")\n",
    "print(\"   2. Uncomment the code block\")\n",
    "print(\"   3. Run the cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effaa3ac",
   "metadata": {},
   "source": [
    "## Panoptic Segmentation Inference Pipeline\n",
    "\n",
    "This section implements the core inference pipeline for running panoptic segmentation on urban scene images. The pipeline combines semantic segmentation (pixel-level classification) with instance segmentation (object detection and segmentation) to provide comprehensive scene understanding.\n",
    "\n",
    "**Inference Process:**\n",
    "1. **Semantic Segmentation**: Classifies each pixel into one of 19 Cityscapes classes\n",
    "2. **Instance Detection**: Identifies and segments individual object instances\n",
    "3. **Panoptic Fusion**: Combines semantic and instance results into unified panoptic segmentation\n",
    "4. **Post-processing**: Applies confidence thresholding and non-maximum suppression\n",
    "\n",
    "**Output Components:**\n",
    "- **Panoptic Mask**: Single mask where each pixel has a unique segment ID\n",
    "- **Segments Info**: Detailed information about each segment (class, area, confidence)\n",
    "- **Semantic Predictions**: Class probabilities for each pixel\n",
    "- **Instance Predictions**: Bounding boxes, masks, and confidence scores for objects\n",
    "\n",
    "The inference pipeline is optimized for batch processing while maintaining detailed tracking of performance metrics and processing times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_panoptic_segmentation(predictor, image):\n",
    "    \"\"\"\n",
    "    Run panoptic segmentation inference on an image\n",
    "    \n",
    "    Args:\n",
    "        predictor: Detectron2 predictor object\n",
    "        image: Input image in BGR format\n",
    "        \n",
    "    Returns:\n",
    "        dict: Prediction results containing panoptic_seg, segments_info, etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run inference\n",
    "        outputs = predictor(image)\n",
    "        \n",
    "        # Extract panoptic segmentation results\n",
    "        panoptic_seg = outputs[\"panoptic_seg\"]\n",
    "        segments_info = outputs[\"segments_info\"]\n",
    "        \n",
    "        print(f\"‚úÖ Inference completed successfully!\")\n",
    "        print(f\"üß© Found {len(segments_info)} segments\")\n",
    "        \n",
    "        return {\n",
    "            \"panoptic_seg\": panoptic_seg,\n",
    "            \"segments_info\": segments_info,\n",
    "            \"full_outputs\": outputs\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during inference: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_panoptic_to_class_mask(panoptic_seg, segments_info, evaluator):\n",
    "    \"\"\"\n",
    "    Convert panoptic segmentation to class mask for evaluation\n",
    "    Following evaluate.py format\n",
    "    \n",
    "    Args:\n",
    "        panoptic_seg: Panoptic segmentation tensor from Detectron2\n",
    "        segments_info: Segment information list\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Class mask with Cityscapes class IDs\n",
    "    \"\"\"\n",
    "    # Convert tensor to numpy if needed\n",
    "    if hasattr(panoptic_seg, 'cpu'):\n",
    "        seg_map = panoptic_seg.cpu().numpy()\n",
    "    else:\n",
    "        seg_map = panoptic_seg\n",
    "    \n",
    "    # Initialize class mask with background\n",
    "    class_mask = np.full(seg_map.shape, evaluator.class_mapping['background'], dtype=np.uint8)\n",
    "    \n",
    "    # Cityscapes category ID to class name mapping (from Detectron2)\n",
    "    cityscapes_id_to_name = {\n",
    "        0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
    "        5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation',\n",
    "        9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car',\n",
    "        14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'\n",
    "    }\n",
    "    \n",
    "    # Create mapping from segment ID to category\n",
    "    segment_id_to_category = {}\n",
    "    for segment in segments_info:\n",
    "        segment_id_to_category[segment[\"id\"]] = segment[\"category_id\"]\n",
    "    \n",
    "    # Assign class labels to each segment\n",
    "    unique_ids = np.unique(seg_map)\n",
    "    for segment_id in unique_ids:\n",
    "        if segment_id in segment_id_to_category:\n",
    "            category_id = segment_id_to_category[segment_id]\n",
    "            if category_id in cityscapes_id_to_name:\n",
    "                class_name = cityscapes_id_to_name[category_id]\n",
    "                if class_name in evaluator.class_mapping:\n",
    "                    class_idx = evaluator.class_mapping[class_name]\n",
    "                    mask = seg_map == segment_id\n",
    "                    class_mask[mask] = class_idx\n",
    "    \n",
    "    return class_mask\n",
    "\n",
    "def analyze_segments(segments_info, evaluator):\n",
    "    \"\"\"\n",
    "    Analyze the detected segments and provide statistics\n",
    "    Updated to work with SegmentationEvaluator\n",
    "    \n",
    "    Args:\n",
    "        segments_info: List of segment information from Detectron2\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        \n",
    "    Returns:\n",
    "        dict: Analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count instances by category\n",
    "    category_counts = {}\n",
    "    total_area = 0\n",
    "    \n",
    "    stuff_segments = 0\n",
    "    thing_segments = 0\n",
    "    \n",
    "    # Cityscapes category ID to class name mapping\n",
    "    cityscapes_id_to_name = {\n",
    "        0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
    "        5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation',\n",
    "        9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car',\n",
    "        14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'\n",
    "    }\n",
    "    \n",
    "    for segment in segments_info:\n",
    "        category_id = segment[\"category_id\"]\n",
    "        area = segment[\"area\"]\n",
    "        \n",
    "        # Get class name\n",
    "        class_name = cityscapes_id_to_name.get(category_id, \"unknown\")\n",
    "        \n",
    "        # Count categories\n",
    "        if class_name not in category_counts:\n",
    "            category_counts[class_name] = {\"count\": 0, \"total_area\": 0}\n",
    "        \n",
    "        category_counts[class_name][\"count\"] += 1\n",
    "        category_counts[class_name][\"total_area\"] += area\n",
    "        total_area += area\n",
    "        \n",
    "        # Count stuff vs thing categories\n",
    "        # In Cityscapes, categories 0-10 are typically \"stuff\" (road, sidewalk, etc.)\n",
    "        # and 11+ are \"things\" (person, car, etc.)\n",
    "        if category_id <= 10:\n",
    "            stuff_segments += 1\n",
    "        else:\n",
    "            thing_segments += 1\n",
    "    \n",
    "    return {\n",
    "        \"category_counts\": category_counts,\n",
    "        \"total_segments\": len(segments_info),\n",
    "        \"stuff_segments\": stuff_segments,\n",
    "        \"thing_segments\": thing_segments,\n",
    "        \"total_area\": total_area\n",
    "    }\n",
    "\n",
    "def print_analysis_results(analysis_results):\n",
    "    \"\"\"Print formatted analysis results\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìä SEGMENTATION ANALYSIS RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"üß© Total segments: {analysis_results['total_segments']}\")\n",
    "    print(f\"üèûÔ∏è  Stuff segments: {analysis_results['stuff_segments']}\")\n",
    "    print(f\"üöó Thing segments: {analysis_results['thing_segments']}\")\n",
    "    print(f\"üìê Total area: {analysis_results['total_area']:,} pixels\")\n",
    "    \n",
    "    print(\"\\nüìã DETECTED CATEGORIES:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Sort categories by area (descending)\n",
    "    sorted_categories = sorted(\n",
    "        analysis_results['category_counts'].items(),\n",
    "        key=lambda x: x[1]['total_area'],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for class_name, info in sorted_categories:\n",
    "        percentage = (info['total_area'] / analysis_results['total_area']) * 100\n",
    "        print(f\"{class_name:15s} | Count: {info['count']:2d} | Area: {percentage:5.1f}%\")\n",
    "\n",
    "def create_dummy_ground_truth(image_shape, segments_info, evaluator):\n",
    "    \"\"\"\n",
    "    Create a dummy ground truth mask for demonstration purposes\n",
    "    In practice, you would load actual ground truth annotations\n",
    "    \n",
    "    Args:\n",
    "        image_shape: Shape of the image (height, width)\n",
    "        segments_info: Segment information from prediction (for reference)\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Dummy ground truth mask\n",
    "    \"\"\"\n",
    "    height, width = image_shape[:2]\n",
    "    \n",
    "    # Create a simple synthetic ground truth for demonstration\n",
    "    gt_mask = np.full((height, width), evaluator.class_mapping['background'], dtype=np.uint8)\n",
    "    \n",
    "    # Add some basic regions (this is just for demo - replace with real GT loading)\n",
    "    # Sky region (upper portion)\n",
    "    gt_mask[:height//4, :] = evaluator.class_mapping['sky']\n",
    "    \n",
    "    # Building region (middle-left)\n",
    "    gt_mask[height//4:3*height//4, :width//3] = evaluator.class_mapping['building']\n",
    "    \n",
    "    # Road region (bottom portion)\n",
    "    gt_mask[3*height//4:, :] = evaluator.class_mapping['road']\n",
    "    \n",
    "    # Vegetation region (middle-right)\n",
    "    gt_mask[height//4:3*height//4, 2*width//3:] = evaluator.class_mapping['vegetation']\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Using dummy ground truth for demonstration\")\n",
    "    print(\"   In practice, load actual ground truth annotations\")\n",
    "    \n",
    "    return gt_mask\n",
    "\n",
    "def evaluate_prediction_with_metrics(predictor, image, evaluator, gt_mask=None):\n",
    "    \"\"\"\n",
    "    Complete evaluation pipeline with mIoU metrics computation\n",
    "    Following evaluate.py structure\n",
    "    \n",
    "    Args:\n",
    "        predictor: Detectron2 predictor object\n",
    "        image: Input image in BGR format\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        gt_mask: Ground truth mask (optional, will create dummy if None)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Complete evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run inference\n",
    "        outputs = predictor(image)\n",
    "        \n",
    "        # Extract panoptic segmentation results\n",
    "        panoptic_seg = outputs[\"panoptic_seg\"]\n",
    "        segments_info = outputs[\"segments_info\"]\n",
    "        \n",
    "        print(f\"Inference completed successfully!\")\n",
    "        print(f\"Found {len(segments_info)} segments\")\n",
    "        \n",
    "        # Convert panoptic segmentation to class mask\n",
    "        pred_class_mask = convert_panoptic_to_class_mask(\n",
    "            panoptic_seg[0], segments_info, evaluator\n",
    "        )\n",
    "        \n",
    "        # Create or use provided ground truth\n",
    "        if gt_mask is None:\n",
    "            gt_mask = create_dummy_ground_truth(image.shape, segments_info, evaluator)\n",
    "        \n",
    "        # Ensure masks have the same shape\n",
    "        if gt_mask.shape != pred_class_mask.shape:\n",
    "            gt_mask = cv2.resize(gt_mask, (pred_class_mask.shape[1], pred_class_mask.shape[0]), \n",
    "                               interpolation=cv2.INTER_NEAREST)\n",
    "        \n",
    "        # Calculate comprehensive metrics\n",
    "        metrics, overall_accuracy, mean_iou_all, mean_iou_key, cm = evaluator.calculate_metrics(\n",
    "            gt_mask, pred_class_mask\n",
    "        )\n",
    "        \n",
    "        # Print metrics summary\n",
    "        evaluator.print_metrics_summary(metrics, overall_accuracy, mean_iou_all, mean_iou_key)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        evaluator.plot_confusion_matrix(cm)\n",
    "        \n",
    "        return {\n",
    "            \"panoptic_seg\": panoptic_seg,\n",
    "            \"segments_info\": segments_info,\n",
    "            \"pred_class_mask\": pred_class_mask,\n",
    "            \"gt_mask\": gt_mask,\n",
    "            \"metrics\": metrics,\n",
    "            \"overall_accuracy\": overall_accuracy,\n",
    "            \"mean_iou_all\": mean_iou_all,\n",
    "            \"mean_iou_key\": mean_iou_key,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"full_outputs\": outputs\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example evaluation workflow (uncomment to run)\n",
    "# if 'predictor' in locals() and 'sample_image' in locals():\n",
    "#     # Run complete evaluation with metrics\n",
    "#     eval_results = evaluate_prediction_with_metrics(predictor, sample_image, evaluator)\n",
    "#     \n",
    "#     if eval_results is not None:\n",
    "#         print(f\"\\nüéØ Key Results:\")\n",
    "#         print(f\"   Overall Accuracy: {eval_results['overall_accuracy']:.4f}\")\n",
    "#         print(f\"   Mean IoU (All): {eval_results['mean_iou_all']:.4f}\")\n",
    "#         print(f\"   Mean IoU (Key Urban): {eval_results['mean_iou_key']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c32efd",
   "metadata": {},
   "source": [
    "## Comprehensive Visualization and Analysis\n",
    "\n",
    "This section provides advanced visualization capabilities for analyzing panoptic segmentation results and understanding model performance patterns. The visualization suite is specifically designed for urban scene analysis and restoration quality assessment.\n",
    "\n",
    "**Visualization Components:**\n",
    "\n",
    "**Segmentation Overlay Visualizations:**\n",
    "- **Color-coded Segmentation Maps**: Visual representation of all detected segments with distinct colors\n",
    "- **Class-specific Highlighting**: Individual visualization of specific urban scene categories\n",
    "- **Instance Boundary Visualization**: Clear delineation between different object instances\n",
    "\n",
    "**Performance Analysis Charts:**\n",
    "- **Per-Class IoU Distribution**: Bar charts showing segmentation quality for each Cityscapes class\n",
    "- **Confusion Matrices**: Heat maps revealing common misclassification patterns between urban scene classes\n",
    "- **Precision-Recall Curves**: Analysis of detection performance across different confidence thresholds\n",
    "\n",
    "**Comparative Analysis:**\n",
    "- **Ground Truth vs Prediction**: Side-by-side comparison highlighting differences\n",
    "- **Error Pattern Visualization**: Systematic identification of failure modes and challenging scenarios\n",
    "- **Scale-based Performance**: Analysis of how segmentation quality varies with object size\n",
    "\n",
    "These visualizations are essential for understanding restoration impact on urban scene understanding and identifying specific areas where image quality affects segmentation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7966fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "import random\n",
    "\n",
    "def create_colored_segmentation_map(panoptic_seg, segments_info, evaluator):\n",
    "    \"\"\"\n",
    "    Create a colored segmentation map using Cityscapes colors\n",
    "    Updated to work with SegmentationEvaluator\n",
    "    \n",
    "    Args:\n",
    "        panoptic_seg: Panoptic segmentation tensor from Detectron2\n",
    "        segments_info: Segment information list\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Colored segmentation map (H, W, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert tensor to numpy if needed\n",
    "    if hasattr(panoptic_seg, 'cpu'):\n",
    "        seg_map = panoptic_seg.cpu().numpy()\n",
    "    else:\n",
    "        seg_map = panoptic_seg\n",
    "    \n",
    "    # Create colored output\n",
    "    height, width = seg_map.shape\n",
    "    colored_map = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Cityscapes category ID to class name mapping\n",
    "    cityscapes_id_to_name = {\n",
    "        0: 'road', 1: 'sidewalk', 2: 'building', 3: 'wall', 4: 'fence',\n",
    "        5: 'pole', 6: 'traffic light', 7: 'traffic sign', 8: 'vegetation',\n",
    "        9: 'terrain', 10: 'sky', 11: 'person', 12: 'rider', 13: 'car',\n",
    "        14: 'truck', 15: 'bus', 16: 'train', 17: 'motorcycle', 18: 'bicycle'\n",
    "    }\n",
    "    \n",
    "    # Create mapping from segment ID to category\n",
    "    segment_id_to_category = {}\n",
    "    for segment in segments_info:\n",
    "        segment_id_to_category[segment[\"id\"]] = segment[\"category_id\"]\n",
    "    \n",
    "    # Color each segment\n",
    "    unique_ids = np.unique(seg_map)\n",
    "    for segment_id in unique_ids:\n",
    "        if segment_id in segment_id_to_category:\n",
    "            category_id = segment_id_to_category[segment_id]\n",
    "            if category_id in cityscapes_id_to_name:\n",
    "                class_name = cityscapes_id_to_name[category_id]\n",
    "                if class_name in evaluator.color_mapping:\n",
    "                    color = evaluator.color_mapping[class_name]\n",
    "                    mask = seg_map == segment_id\n",
    "                    colored_map[mask] = color\n",
    "    \n",
    "    return colored_map\n",
    "\n",
    "def create_colored_class_mask(class_mask, evaluator):\n",
    "    \"\"\"\n",
    "    Create colored visualization from class mask\n",
    "    \n",
    "    Args:\n",
    "        class_mask: Class mask with class indices\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        \n",
    "    Returns:\n",
    "        numpy array: Colored mask (H, W, 3)\n",
    "    \"\"\"\n",
    "    height, width = class_mask.shape\n",
    "    colored_map = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(evaluator.class_names):\n",
    "        if class_name in evaluator.color_mapping:\n",
    "            color = evaluator.color_mapping[class_name]\n",
    "            mask = class_mask == class_idx\n",
    "            colored_map[mask] = color\n",
    "    \n",
    "    return colored_map\n",
    "\n",
    "def visualize_evaluation_results(image, eval_results, evaluator, alpha=0.6):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of evaluation results including metrics\n",
    "    Updated to work with evaluation structure\n",
    "    \n",
    "    Args:\n",
    "        image: Original input image (BGR format)\n",
    "        eval_results: Results dictionary from evaluate_prediction_with_metrics\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        alpha: Transparency for overlay (0.0 = transparent, 1.0 = opaque)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing different visualization outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert BGR to RGB for matplotlib\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Get components from evaluation results\n",
    "    panoptic_seg = eval_results[\"panoptic_seg\"][0]  # Remove batch dimension\n",
    "    segments_info = eval_results[\"segments_info\"]\n",
    "    pred_class_mask = eval_results[\"pred_class_mask\"]\n",
    "    gt_mask = eval_results[\"gt_mask\"]\n",
    "    \n",
    "    # Create colored visualizations\n",
    "    colored_pred = create_colored_segmentation_map(panoptic_seg, segments_info, evaluator)\n",
    "    colored_gt = create_colored_class_mask(gt_mask, evaluator)\n",
    "    colored_pred_class = create_colored_class_mask(pred_class_mask, evaluator)\n",
    "    \n",
    "    # Create overlays\n",
    "    pred_overlay = cv2.addWeighted(image_rgb, 1-alpha, colored_pred, alpha, 0)\n",
    "    gt_overlay = cv2.addWeighted(image_rgb, 1-alpha, colored_gt, alpha, 0)\n",
    "    \n",
    "    # Create difference map (for error analysis)\n",
    "    diff_mask = (gt_mask != pred_class_mask).astype(np.uint8) * 255\n",
    "    diff_colored = np.zeros_like(image_rgb)\n",
    "    diff_colored[:, :, 0] = diff_mask  # Red channel for errors\n",
    "    diff_overlay = cv2.addWeighted(image_rgb, 0.7, diff_colored, 0.3, 0)\n",
    "    \n",
    "    return {\n",
    "        \"original\": image_rgb,\n",
    "        \"prediction_panoptic\": colored_pred,\n",
    "        \"prediction_class\": colored_pred_class,\n",
    "        \"ground_truth\": colored_gt,\n",
    "        \"pred_overlay\": pred_overlay,\n",
    "        \"gt_overlay\": gt_overlay,\n",
    "        \"difference_map\": diff_overlay,\n",
    "        \"metrics\": eval_results[\"metrics\"],\n",
    "        \"overall_accuracy\": eval_results[\"overall_accuracy\"],\n",
    "        \"mean_iou_all\": eval_results[\"mean_iou_all\"],\n",
    "        \"mean_iou_key\": eval_results[\"mean_iou_key\"]\n",
    "    }\n",
    "\n",
    "def plot_comprehensive_evaluation_results(vis_results, eval_results, evaluator, figsize=(24, 18)):\n",
    "    \"\"\"\n",
    "    Create a comprehensive evaluation visualization following evaluate.py format\n",
    "    \n",
    "    Args:\n",
    "        vis_results: Results from visualize_evaluation_results\n",
    "        eval_results: Results from evaluate_prediction_with_metrics\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        figsize: Figure size tuple\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, 4, figsize=figsize)\n",
    "    fig.suptitle('Comprehensive Urban Scene Segmentation Evaluation', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # Row 1: Original, Prediction, Ground Truth, Difference\n",
    "    axes[0, 0].imshow(vis_results[\"original\"])\n",
    "    axes[0, 0].set_title('Original Image', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(vis_results[\"prediction_panoptic\"])\n",
    "    axes[0, 1].set_title('Prediction (Panoptic)', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(vis_results[\"ground_truth\"])\n",
    "    axes[0, 2].set_title('Ground Truth', fontweight='bold', fontsize=12)\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[0, 3].imshow(vis_results[\"difference_map\"])\n",
    "    axes[0, 3].set_title('Difference Map (Errors in Red)', fontweight='bold', fontsize=12)\n",
    "    axes[0, 3].axis('off')\n",
    "    \n",
    "    # Row 2: Overlays and IoU Chart\n",
    "    axes[1, 0].imshow(vis_results[\"pred_overlay\"])\n",
    "    axes[1, 0].set_title('Prediction Overlay', fontweight='bold', fontsize=12)\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(vis_results[\"gt_overlay\"])\n",
    "    axes[1, 1].set_title('Ground Truth Overlay', fontweight='bold', fontsize=12)\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    # IoU bar chart\n",
    "    metrics = vis_results[\"metrics\"]\n",
    "    classes_with_data = [cls for cls in evaluator.class_names if cls in metrics and metrics[cls]['support'] > 0]\n",
    "    ious = [metrics[cls]['iou'] for cls in classes_with_data]\n",
    "    \n",
    "    bars = axes[1, 2].bar(range(len(classes_with_data)), ious, alpha=0.7, color='skyblue')\n",
    "    axes[1, 2].set_title('Per-Class IoU Scores', fontweight='bold', fontsize=12)\n",
    "    axes[1, 2].set_xticks(range(len(classes_with_data)))\n",
    "    axes[1, 2].set_xticklabels([cls[:8] for cls in classes_with_data], rotation=45, ha='right', fontsize=10)\n",
    "    axes[1, 2].set_ylabel('IoU Score', fontsize=10)\n",
    "    axes[1, 2].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, iou in zip(bars, ious):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                       f'{iou:.2f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Metrics summary text\n",
    "    summary_text = f\"\"\"EVALUATION METRICS\n",
    "    \n",
    "Overall Accuracy: {vis_results['overall_accuracy']:.3f}\n",
    "Mean IoU (All): {vis_results['mean_iou_all']:.3f}\n",
    "Mean IoU (Key Urban): {vis_results['mean_iou_key']:.3f}\n",
    "\n",
    "TOP PERFORMING CLASSES:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add top 5 classes by IoU\n",
    "    sorted_metrics = sorted(\n",
    "        [(cls, metrics[cls]['iou']) for cls in classes_with_data],\n",
    "        key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "    \n",
    "    for i, (cls_name, iou_score) in enumerate(sorted_metrics[:5]):\n",
    "        summary_text += f\"\\n{i+1}. {cls_name}: {iou_score:.3f}\"\n",
    "    \n",
    "    axes[1, 3].text(0.05, 0.95, summary_text, transform=axes[1, 3].transAxes,\n",
    "                   verticalalignment='top', fontfamily='monospace', fontsize=10,\n",
    "                   bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    axes[1, 3].set_title('Metrics Summary', fontweight='bold', fontsize=12)\n",
    "    axes[1, 3].axis('off')\n",
    "    \n",
    "    # Row 3: Precision, Recall, F1 scores, and Support\n",
    "    precision_scores = [metrics[cls]['precision'] for cls in classes_with_data]\n",
    "    recall_scores = [metrics[cls]['recall'] for cls in classes_with_data]\n",
    "    f1_scores = [metrics[cls]['f1'] for cls in classes_with_data]\n",
    "    supports = [metrics[cls]['support'] for cls in classes_with_data]\n",
    "    \n",
    "    # Precision chart\n",
    "    axes[2, 0].bar(range(len(classes_with_data)), precision_scores, alpha=0.7, color='lightgreen')\n",
    "    axes[2, 0].set_title('Precision Scores', fontweight='bold', fontsize=12)\n",
    "    axes[2, 0].set_xticks(range(len(classes_with_data)))\n",
    "    axes[2, 0].set_xticklabels([cls[:8] for cls in classes_with_data], rotation=45, ha='right', fontsize=10)\n",
    "    axes[2, 0].set_ylabel('Precision', fontsize=10)\n",
    "    axes[2, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Recall chart\n",
    "    axes[2, 1].bar(range(len(classes_with_data)), recall_scores, alpha=0.7, color='lightcoral')\n",
    "    axes[2, 1].set_title('Recall Scores', fontweight='bold', fontsize=12)\n",
    "    axes[2, 1].set_xticks(range(len(classes_with_data)))\n",
    "    axes[2, 1].set_xticklabels([cls[:8] for cls in classes_with_data], rotation=45, ha='right', fontsize=10)\n",
    "    axes[2, 1].set_ylabel('Recall', fontsize=10)\n",
    "    axes[2, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # F1 chart\n",
    "    axes[2, 2].bar(range(len(classes_with_data)), f1_scores, alpha=0.7, color='gold')\n",
    "    axes[2, 2].set_title('F1 Scores', fontweight='bold', fontsize=12)\n",
    "    axes[2, 2].set_xticks(range(len(classes_with_data)))\n",
    "    axes[2, 2].set_xticklabels([cls[:8] for cls in classes_with_data], rotation=45, ha='right', fontsize=10)\n",
    "    axes[2, 2].set_ylabel('F1 Score', fontsize=10)\n",
    "    axes[2, 2].set_ylim(0, 1)\n",
    "    \n",
    "    # Support (log scale for better visualization)\n",
    "    axes[2, 3].bar(range(len(classes_with_data)), supports, alpha=0.7, color='mediumpurple')\n",
    "    axes[2, 3].set_title('Support (Pixel Count)', fontweight='bold', fontsize=12)\n",
    "    axes[2, 3].set_xticks(range(len(classes_with_data)))\n",
    "    axes[2, 3].set_xticklabels([cls[:8] for cls in classes_with_data], rotation=45, ha='right', fontsize=10)\n",
    "    axes[2, 3].set_ylabel('Support (log scale)', fontsize=10)\n",
    "    axes[2, 3].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Also plot the confusion matrix separately for better visibility\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    evaluator.plot_confusion_matrix(eval_results[\"confusion_matrix\"])\n",
    "\n",
    "# Example comprehensive evaluation workflow (uncomment to run)\n",
    "# if 'predictor' in locals() and 'sample_image' in locals():\n",
    "#     # Run complete evaluation with metrics\n",
    "#     eval_results = evaluate_prediction_with_metrics(predictor, sample_image, evaluator)\n",
    "#     \n",
    "#     if eval_results is not None:\n",
    "#         # Create comprehensive visualizations\n",
    "#         vis_results = visualize_evaluation_results(sample_image, eval_results, evaluator)\n",
    "#         \n",
    "#         # Plot all results\n",
    "#         plot_comprehensive_evaluation_results(vis_results, eval_results, evaluator)\n",
    "\n",
    "# Example visualization workflow (uncomment to run)\n",
    "# if 'results' in locals() and 'sample_image' in locals():\n",
    "#     # Create visualizations\n",
    "#     vis_results = visualize_results(\n",
    "#         sample_image, results, cityscapes_classes, cityscapes_colors\n",
    "#     )\n",
    "#     \n",
    "#     # Create comprehensive plot\n",
    "#     plot_results_grid(vis_results, analysis, cityscapes_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45295169",
   "metadata": {},
   "source": [
    "## Batch Processing and Dataset Evaluation\n",
    "\n",
    "This section implements efficient batch processing capabilities for evaluating panoptic segmentation performance across entire datasets. The batch processing system is designed to handle large-scale evaluation while maintaining detailed per-image analysis and aggregate statistics.\n",
    "\n",
    "**Batch Processing Features:**\n",
    "\n",
    "**Scalable Processing Pipeline:**\n",
    "- **Parallel Processing**: Multi-threaded image processing for faster evaluation\n",
    "- **Memory Management**: Efficient memory usage for processing large image datasets\n",
    "- **Progress Tracking**: Real-time monitoring of processing status and estimated completion times\n",
    "- **Error Handling**: Robust handling of corrupted images and processing failures\n",
    "\n",
    "**Comprehensive Output Generation:**\n",
    "- **Individual Results**: Detailed per-image segmentation results and metrics\n",
    "- **Aggregate Statistics**: Dataset-wide performance summaries and distributions\n",
    "- **Export Formats**: Multiple output formats including JSON, CSV, and visualization files\n",
    "- **Comparative Analysis**: Statistical comparisons across different image subsets or conditions\n",
    "\n",
    "**Performance Monitoring:**\n",
    "- **Processing Speed**: Tracking of inference times and throughput rates\n",
    "- **Resource Utilization**: Monitoring of GPU/CPU usage and memory consumption\n",
    "- **Quality Metrics**: Continuous computation of segmentation quality indicators\n",
    "\n",
    "The batch processing system enables comprehensive evaluation of restoration effects on urban scene understanding across diverse datasets and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6ba4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def process_batch_with_evaluation(predictor, image_files, output_dir, evaluator,\n",
    "                                 save_visualizations=True, save_masks=True, save_json=True, save_metrics=True):\n",
    "    \"\"\"\n",
    "    Process multiple images in batch with comprehensive evaluation and metrics\n",
    "    Following evaluate.py structure with mIoU computation\n",
    "    \n",
    "    Args:\n",
    "        predictor: Detectron2 predictor object\n",
    "        image_files: List of image file paths\n",
    "        output_dir: Directory to save results\n",
    "        evaluator: SegmentationEvaluator instance\n",
    "        save_visualizations: Whether to save visualization images\n",
    "        save_masks: Whether to save segmentation masks\n",
    "        save_json: Whether to save JSON results\n",
    "        save_metrics: Whether to save detailed metrics\n",
    "        \n",
    "    Returns:\n",
    "        dict: Summary of batch processing results with metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create output directories\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if save_visualizations:\n",
    "        (output_dir / \"visualizations\").mkdir(exist_ok=True)\n",
    "    if save_masks:\n",
    "        (output_dir / \"masks\").mkdir(exist_ok=True)\n",
    "    if save_json:\n",
    "        (output_dir / \"json\").mkdir(exist_ok=True)\n",
    "    \n",
    "    batch_results = []\n",
    "    processing_times = []\n",
    "    \n",
    "    print(f\"Processing {len(image_files)} images...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, image_path in enumerate(tqdm(image_files, desc=\"Processing images\")):\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Load image\n",
    "            image = load_image(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            # Get base filename\n",
    "            base_name = Path(image_path).stem\n",
    "            \n",
    "            # Run inference\n",
    "            results = run_panoptic_segmentation(predictor, image)\n",
    "            if results is None:\n",
    "                continue\n",
    "            \n",
    "            # Analyze results\n",
    "            analysis = analyze_segments(results[\"segments_info\"], cityscapes_classes)\n",
    "            \n",
    "            # Create visualizations\n",
    "            if save_visualizations or save_masks:\n",
    "                vis_results = visualize_results(\n",
    "                    image, results, cityscapes_classes, cityscapes_colors\n",
    "                )\n",
    "                \n",
    "                # Save visualization\n",
    "                if save_visualizations:\n",
    "                    vis_path = output_dir / \"visualizations\" / f\"{base_name}_visualization.png\"\n",
    "                    plt.figure(figsize=(15, 5))\n",
    "                    \n",
    "                    plt.subplot(1, 3, 1)\n",
    "                    plt.imshow(vis_results[\"original\"])\n",
    "                    plt.title(\"Original\")\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    plt.subplot(1, 3, 2)\n",
    "                    plt.imshow(vis_results[\"segmentation_map\"])\n",
    "                    plt.title(\"Segmentation\")\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    plt.subplot(1, 3, 3)\n",
    "                    plt.imshow(vis_results[\"overlay\"])\n",
    "                    plt.title(\"Overlay\")\n",
    "                    plt.axis('off')\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.savefig(vis_path, dpi=150, bbox_inches='tight')\n",
    "                    plt.close()\n",
    "                \n",
    "                # Save mask\n",
    "                if save_masks:\n",
    "                    mask_path = output_dir / \"masks\" / f\"{base_name}_mask.png\"\n",
    "                    cv2.imwrite(str(mask_path), cv2.cvtColor(vis_results[\"segmentation_map\"], cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            # Save JSON results\n",
    "            if save_json:\n",
    "                json_data = {\n",
    "                    \"image_path\": str(image_path),\n",
    "                    \"image_name\": base_name,\n",
    "                    \"analysis\": analysis,\n",
    "                    \"segments_info\": results[\"segments_info\"],\n",
    "                    \"processing_time\": time.time() - start_time\n",
    "                }\n",
    "                \n",
    "                json_path = output_dir / \"json\" / f\"{base_name}_results.json\"\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(json_data, f, indent=2, default=str)\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            processing_times.append(processing_time)\n",
    "            \n",
    "            batch_results.append({\n",
    "                \"image_path\": image_path,\n",
    "                \"base_name\": base_name,\n",
    "                \"analysis\": analysis,\n",
    "                \"processing_time\": processing_time\n",
    "            })\n",
    "            \n",
    "            # Print progress every 10 images\n",
    "            if (i + 1) % 10 == 0:\n",
    "                avg_time = np.mean(processing_times[-10:])\n",
    "                print(f\"‚úÖ Processed {i+1}/{len(image_files)} images (avg: {avg_time:.2f}s/image)\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {image_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Create summary\n",
    "    summary = create_batch_summary(batch_results, output_dir)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def create_batch_summary(batch_results, output_dir):\n",
    "    \"\"\"Create and save a summary of batch processing results\"\"\"\n",
    "    \n",
    "    if not batch_results:\n",
    "        return {\"error\": \"No images were successfully processed\"}\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_images = len(batch_results)\n",
    "    total_time = sum(r[\"processing_time\"] for r in batch_results)\n",
    "    avg_time = total_time / total_images\n",
    "    \n",
    "    # Aggregate category statistics\n",
    "    all_categories = {}\n",
    "    total_segments = 0\n",
    "    \n",
    "    for result in batch_results:\n",
    "        analysis = result[\"analysis\"]\n",
    "        total_segments += analysis[\"total_segments\"]\n",
    "        \n",
    "        for category, info in analysis[\"category_counts\"].items():\n",
    "            if category not in all_categories:\n",
    "                all_categories[category] = {\"total_count\": 0, \"total_area\": 0, \"image_count\": 0}\n",
    "            \n",
    "            all_categories[category][\"total_count\"] += info[\"count\"]\n",
    "            all_categories[category][\"total_area\"] += info[\"total_area\"]\n",
    "            all_categories[category][\"image_count\"] += 1\n",
    "    \n",
    "    summary = {\n",
    "        \"batch_info\": {\n",
    "            \"total_images_processed\": total_images,\n",
    "            \"total_processing_time\": total_time,\n",
    "            \"average_time_per_image\": avg_time,\n",
    "            \"total_segments_found\": total_segments\n",
    "        },\n",
    "        \"category_statistics\": all_categories,\n",
    "        \"per_image_results\": batch_results\n",
    "    }\n",
    "    \n",
    "    # Save summary\n",
    "    summary_path = output_dir / \"batch_summary.json\"\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nBatch processing completed!\")\n",
    "    print(f\"Results saved to: {output_dir}\")\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "    print(f\"Total time: {total_time:.2f}s ({avg_time:.2f}s per image)\")\n",
    "    print(f\"Total segments found: {total_segments}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example batch processing (uncomment to run)\n",
    "# if 'predictor' in locals():\n",
    "#     # Define input and output paths\n",
    "#     input_directory = \"path/to/your/input/images\"\n",
    "#     output_directory = \"path/to/your/output\"\n",
    "#     \n",
    "#     # Get all image files\n",
    "#     image_files = get_image_files(input_directory)[:5]  # Process first 5 images as example\n",
    "#     \n",
    "#     # Process batch\n",
    "#     if image_files:\n",
    "#         summary = process_batch(\n",
    "#             predictor, image_files, output_directory,\n",
    "#             cityscapes_classes, cityscapes_colors,\n",
    "#             save_visualizations=True,\n",
    "#             save_masks=True,\n",
    "#             save_json=True\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839dcf08",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline Demonstration\n",
    "\n",
    "This section provides a complete end-to-end demonstration of the urban scene panoptic segmentation evaluation pipeline. The example showcases the full workflow from model setup through comprehensive evaluation and visualization, following best practices for systematic evaluation.\n",
    "\n",
    "**Complete Pipeline Components:**\n",
    "\n",
    "**Model Configuration and Setup:**\n",
    "- **Architecture Selection**: Choice between different PanopticFCN configurations based on accuracy vs speed requirements\n",
    "- **Confidence Thresholding**: Optimization of detection confidence thresholds for deployment scenarios\n",
    "- **Hardware Optimization**: GPU/CPU configuration for optimal performance\n",
    "\n",
    "**Comprehensive Evaluation Workflow:**\n",
    "1. **Image Preprocessing**: Standard preprocessing pipeline ensuring consistent input formatting\n",
    "2. **Inference Execution**: Panoptic segmentation inference with timing and resource monitoring\n",
    "3. **Results Processing**: Extraction and formatting of segmentation results for analysis\n",
    "4. **Metrics Computation**: Calculation of all evaluation metrics including PQ, SQ, RQ, mIoU, and class-specific scores\n",
    "5. **Visualization Generation**: Creation of comprehensive visual analysis including overlays, charts, and comparison plots\n",
    "6. **Report Generation**: Automated generation of detailed evaluation reports and summaries\n",
    "\n",
    "**Key Evaluation Insights:**\n",
    "The complete pipeline provides actionable insights for assessing restoration quality impact on urban scene understanding, including identification of performance bottlenecks, challenging scene categories, and recommendations for deployment optimization.\n",
    "\n",
    "This comprehensive evaluation serves as a benchmark for determining whether restored urban images maintain sufficient visual fidelity for accurate autonomous driving, urban planning, and smart city applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Urban Scene Panoptic Segmentation Evaluation Pipeline\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"URBAN SCENE PANOPTIC SEGMENTATION EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Set up the model and evaluator\n",
    "print(\"1. Setting up PanopticFCN model and evaluation framework...\")\n",
    "try:\n",
    "    predictor, cfg = setup_panoptic_model(\"PanopticFCN-R50\", confidence_threshold=0.5)\n",
    "    print(\"   Model setup completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"   Model setup failed: {e}\")\n",
    "    print(\"   Please ensure Detectron2 is properly installed\")\n",
    "\n",
    "# 2. Load and process an image\n",
    "print(\"\\n2. Loading and preprocessing urban scene image...\")\n",
    "image_path = \"path/to/your/urban/image.jpg\"  # Update with your image path\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    image = load_image(image_path)\n",
    "    if image is not None:\n",
    "        print(f\"   Image loaded successfully: {image.shape}\")\n",
    "    else:\n",
    "        print(\"   Failed to load image\")\n",
    "else:\n",
    "    print(\"   Image path not found - update the path above\")\n",
    "    print(\"   Creating sample evaluation workflow for demonstration...\")\n",
    "\n",
    "# 3. Run complete evaluation with metrics\n",
    "print(\"\\n3. Running panoptic segmentation evaluation...\")\n",
    "if 'predictor' in locals() and 'image' in locals():\n",
    "    try:\n",
    "        eval_results = evaluate_prediction_with_metrics(predictor, image, evaluator)\n",
    "        \n",
    "        if eval_results is not None:\n",
    "            print(\"   Evaluation completed successfully\")\n",
    "            \n",
    "            # 4. Create comprehensive visualizations\n",
    "            print(\"\\n4. Generating comprehensive visualizations...\")\n",
    "            vis_results = visualize_evaluation_results(image, eval_results, evaluator)\n",
    "            plot_comprehensive_evaluation_results(vis_results, eval_results, evaluator)\n",
    "            \n",
    "            # Print key metrics\n",
    "            print(f\"\\nKEY EVALUATION RESULTS:\")\n",
    "            print(f\"   Overall Accuracy: {eval_results['overall_accuracy']:.4f}\")\n",
    "            print(f\"   Mean IoU (All Classes): {eval_results['mean_iou_all']:.4f}\")\n",
    "            print(f\"   Mean IoU (Key Urban Classes): {eval_results['mean_iou_key']:.4f}\")\n",
    "            print(f\"   Total Segments Detected: {len(eval_results['segments_info'])}\")\n",
    "            \n",
    "            # Summary of class performance\n",
    "            metrics = eval_results['metrics']\n",
    "            best_classes = sorted(metrics.items(), key=lambda x: x[1]['iou'], reverse=True)[:5]\n",
    "            worst_classes = sorted(metrics.items(), key=lambda x: x[1]['iou'])[:5]\n",
    "            \n",
    "            print(f\"\\nTOP 5 PERFORMING CLASSES:\")\n",
    "            for i, (class_name, class_metrics) in enumerate(best_classes, 1):\n",
    "                if class_metrics['support'] > 0:\n",
    "                    print(f\"   {i}. {class_name}: IoU = {class_metrics['iou']:.4f}\")\n",
    "            \n",
    "            print(f\"\\nCHALLENGING CLASSES (Lowest IoU):\")\n",
    "            for i, (class_name, class_metrics) in enumerate(worst_classes, 1):\n",
    "                if class_metrics['support'] > 0:\n",
    "                    print(f\"   {i}. {class_name}: IoU = {class_metrics['iou']:.4f}\")\n",
    "                    \n",
    "        else:\n",
    "            print(\"   Evaluation failed - check input image and model setup\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Evaluation error: {e}\")\n",
    "else:\n",
    "    print(\"   Skipping evaluation - model or image not available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION PIPELINE COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nThis completes the downstream evaluation for urban scene panoptic segmentation.\")\n",
    "print(\"Use the generated metrics and visualizations to assess restoration model quality\")\n",
    "print(\"and determine impact on autonomous driving and urban planning applications.\")\n",
    "\n",
    "print(\"\\nNext steps for analysis:\")\n",
    "print(\"1. Compare mIoU scores across different restoration methods\")\n",
    "print(\"2. Analyze per-class performance to identify restoration-sensitive categories\")\n",
    "print(\"3. Examine boundary quality for safety-critical applications\")\n",
    "print(\"4. Use batch processing for comprehensive dataset evaluation\")\n",
    "print(\"\\nFor deployment considerations:\")\n",
    "print(\"- Overall Accuracy > 90% recommended for autonomous driving applications\")\n",
    "print(\"- Mean IoU > 70% suggested for urban planning and analysis\")\n",
    "print(\"- Pay special attention to 'car', 'person', and 'road' class performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
